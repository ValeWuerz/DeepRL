{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE RANDOM DATA POINTS\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy\n",
    "\n",
    "def blob_label(y, label, loc): # assign labels\n",
    "    target = numpy.copy(y)\n",
    "    for l in loc:\n",
    "        target[y == l] = label\n",
    "    return target\n",
    "\n",
    "\n",
    "x_train, y_train = make_blobs(n_samples=40, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 0, [0]))\n",
    "y_train = torch.FloatTensor(blob_label(y_train, 1, [1,2,3]))\n",
    "x_test, y_test = make_blobs(n_samples=10, n_features=2, cluster_std=1.5, shuffle=True)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 0, [0]))\n",
    "y_test = torch.FloatTensor(blob_label(y_test, 1, [1,2,3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            output = self.sigmoid(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedforward(\n",
       "  (fc1): Linear(in_features=2, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Feedforward(2, 10)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss before training 0.8845343589782715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6240],\n",
       "        [0.6731],\n",
       "        [0.8136],\n",
       "        [0.5457],\n",
       "        [0.6481],\n",
       "        [0.7278],\n",
       "        [0.5683],\n",
       "        [0.5586],\n",
       "        [0.6140],\n",
       "        [0.7982]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "before_train = criterion(y_pred.squeeze(), y_test)\n",
    "print('Test loss before training' , before_train.item())\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.8636641502380371\n",
      "Epoch 1: train loss: 0.8431320190429688\n",
      "Epoch 2: train loss: 0.8238003849983215\n",
      "Epoch 3: train loss: 0.8055909872055054\n",
      "Epoch 4: train loss: 0.7884200811386108\n",
      "Epoch 5: train loss: 0.7722110748291016\n",
      "Epoch 6: train loss: 0.7568933367729187\n",
      "Epoch 7: train loss: 0.742402195930481\n",
      "Epoch 8: train loss: 0.7286781072616577\n",
      "Epoch 9: train loss: 0.7156662940979004\n",
      "Epoch 10: train loss: 0.7033165693283081\n",
      "Epoch 11: train loss: 0.6915823221206665\n",
      "Epoch 12: train loss: 0.6804208755493164\n",
      "Epoch 13: train loss: 0.6697958707809448\n",
      "Epoch 14: train loss: 0.6597252488136292\n",
      "Epoch 15: train loss: 0.6501171588897705\n",
      "Epoch 16: train loss: 0.6409404277801514\n",
      "Epoch 17: train loss: 0.6321665048599243\n",
      "Epoch 18: train loss: 0.6237689256668091\n",
      "Epoch 19: train loss: 0.6157230138778687\n",
      "Epoch 20: train loss: 0.6080060005187988\n",
      "Epoch 21: train loss: 0.600601315498352\n",
      "Epoch 22: train loss: 0.5935879945755005\n",
      "Epoch 23: train loss: 0.5868390798568726\n",
      "Epoch 24: train loss: 0.5803386569023132\n",
      "Epoch 25: train loss: 0.5740712881088257\n",
      "Epoch 26: train loss: 0.5680227875709534\n",
      "Epoch 27: train loss: 0.5621800422668457\n",
      "Epoch 28: train loss: 0.5565334558486938\n",
      "Epoch 29: train loss: 0.5511072874069214\n",
      "Epoch 30: train loss: 0.5458523631095886\n",
      "Epoch 31: train loss: 0.5407584309577942\n",
      "Epoch 32: train loss: 0.5358162522315979\n",
      "Epoch 33: train loss: 0.531017005443573\n",
      "Epoch 34: train loss: 0.5263795852661133\n",
      "Epoch 35: train loss: 0.5219312906265259\n",
      "Epoch 36: train loss: 0.5175595879554749\n",
      "Epoch 37: train loss: 0.5132763981819153\n",
      "Epoch 38: train loss: 0.5090546607971191\n",
      "Epoch 39: train loss: 0.5049115419387817\n",
      "Epoch 40: train loss: 0.5007912516593933\n",
      "Epoch 41: train loss: 0.4965975880622864\n",
      "Epoch 42: train loss: 0.49240702390670776\n",
      "Epoch 43: train loss: 0.4882049560546875\n",
      "Epoch 44: train loss: 0.4840034544467926\n",
      "Epoch 45: train loss: 0.47948598861694336\n",
      "Epoch 46: train loss: 0.47488832473754883\n",
      "Epoch 47: train loss: 0.4703250527381897\n",
      "Epoch 48: train loss: 0.46554651856422424\n",
      "Epoch 49: train loss: 0.4607696533203125\n",
      "Epoch 50: train loss: 0.4559272229671478\n",
      "Epoch 51: train loss: 0.4511687755584717\n",
      "Epoch 52: train loss: 0.4464787542819977\n",
      "Epoch 53: train loss: 0.4419228136539459\n",
      "Epoch 54: train loss: 0.43747034668922424\n",
      "Epoch 55: train loss: 0.4330703616142273\n",
      "Epoch 56: train loss: 0.4287155568599701\n",
      "Epoch 57: train loss: 0.42440205812454224\n",
      "Epoch 58: train loss: 0.4201539158821106\n",
      "Epoch 59: train loss: 0.41595402359962463\n",
      "Epoch 60: train loss: 0.41178521513938904\n",
      "Epoch 61: train loss: 0.40766048431396484\n",
      "Epoch 62: train loss: 0.4035707116127014\n",
      "Epoch 63: train loss: 0.3995213806629181\n",
      "Epoch 64: train loss: 0.3954715132713318\n",
      "Epoch 65: train loss: 0.3914172649383545\n",
      "Epoch 66: train loss: 0.3873549997806549\n",
      "Epoch 67: train loss: 0.3832998275756836\n",
      "Epoch 68: train loss: 0.3792605996131897\n",
      "Epoch 69: train loss: 0.37520676851272583\n",
      "Epoch 70: train loss: 0.3711693286895752\n",
      "Epoch 71: train loss: 0.36710935831069946\n",
      "Epoch 72: train loss: 0.36302894353866577\n",
      "Epoch 73: train loss: 0.35896262526512146\n",
      "Epoch 74: train loss: 0.35491180419921875\n",
      "Epoch 75: train loss: 0.35083478689193726\n",
      "Epoch 76: train loss: 0.346731960773468\n",
      "Epoch 77: train loss: 0.342598557472229\n",
      "Epoch 78: train loss: 0.3384340703487396\n",
      "Epoch 79: train loss: 0.33423855900764465\n",
      "Epoch 80: train loss: 0.33001214265823364\n",
      "Epoch 81: train loss: 0.3257552683353424\n",
      "Epoch 82: train loss: 0.3214687705039978\n",
      "Epoch 83: train loss: 0.31715455651283264\n",
      "Epoch 84: train loss: 0.31281328201293945\n",
      "Epoch 85: train loss: 0.30844688415527344\n",
      "Epoch 86: train loss: 0.30405646562576294\n",
      "Epoch 87: train loss: 0.29964590072631836\n",
      "Epoch 88: train loss: 0.29521575570106506\n",
      "Epoch 89: train loss: 0.29077041149139404\n",
      "Epoch 90: train loss: 0.2863120138645172\n",
      "Epoch 91: train loss: 0.281853049993515\n",
      "Epoch 92: train loss: 0.2774352431297302\n",
      "Epoch 93: train loss: 0.27301210165023804\n",
      "Epoch 94: train loss: 0.2685887813568115\n",
      "Epoch 95: train loss: 0.264167845249176\n",
      "Epoch 96: train loss: 0.2597702145576477\n",
      "Epoch 97: train loss: 0.2554214596748352\n",
      "Epoch 98: train loss: 0.251108855009079\n",
      "Epoch 99: train loss: 0.24681401252746582\n",
      "Epoch 100: train loss: 0.2425394058227539\n",
      "Epoch 101: train loss: 0.23828987777233124\n",
      "Epoch 102: train loss: 0.23406949639320374\n",
      "Epoch 103: train loss: 0.22990083694458008\n",
      "Epoch 104: train loss: 0.22579307854175568\n",
      "Epoch 105: train loss: 0.22174115478992462\n",
      "Epoch 106: train loss: 0.21773040294647217\n",
      "Epoch 107: train loss: 0.213764950633049\n",
      "Epoch 108: train loss: 0.2098597288131714\n",
      "Epoch 109: train loss: 0.20601871609687805\n",
      "Epoch 110: train loss: 0.20223233103752136\n",
      "Epoch 111: train loss: 0.19851775467395782\n",
      "Epoch 112: train loss: 0.1948598325252533\n",
      "Epoch 113: train loss: 0.19126100838184357\n",
      "Epoch 114: train loss: 0.18772220611572266\n",
      "Epoch 115: train loss: 0.1842455416917801\n",
      "Epoch 116: train loss: 0.1808333396911621\n",
      "Epoch 117: train loss: 0.17752650380134583\n",
      "Epoch 118: train loss: 0.1742827296257019\n",
      "Epoch 119: train loss: 0.17110665142536163\n",
      "Epoch 120: train loss: 0.16800127923488617\n",
      "Epoch 121: train loss: 0.16495999693870544\n",
      "Epoch 122: train loss: 0.16198337078094482\n",
      "Epoch 123: train loss: 0.15907151997089386\n",
      "Epoch 124: train loss: 0.1562245786190033\n",
      "Epoch 125: train loss: 0.15344183146953583\n",
      "Epoch 126: train loss: 0.15072320401668549\n",
      "Epoch 127: train loss: 0.14806821942329407\n",
      "Epoch 128: train loss: 0.1454763114452362\n",
      "Epoch 129: train loss: 0.14294689893722534\n",
      "Epoch 130: train loss: 0.1404789388179779\n",
      "Epoch 131: train loss: 0.13807165622711182\n",
      "Epoch 132: train loss: 0.13572409749031067\n",
      "Epoch 133: train loss: 0.13343532383441925\n",
      "Epoch 134: train loss: 0.13120418787002563\n",
      "Epoch 135: train loss: 0.12903568148612976\n",
      "Epoch 136: train loss: 0.1269286870956421\n",
      "Epoch 137: train loss: 0.12487528473138809\n",
      "Epoch 138: train loss: 0.12287372350692749\n",
      "Epoch 139: train loss: 0.12092344462871552\n",
      "Epoch 140: train loss: 0.11902326345443726\n",
      "Epoch 141: train loss: 0.11717192828655243\n",
      "Epoch 142: train loss: 0.11536818742752075\n",
      "Epoch 143: train loss: 0.11361082643270493\n",
      "Epoch 144: train loss: 0.11189858615398407\n",
      "Epoch 145: train loss: 0.11023028939962387\n",
      "Epoch 146: train loss: 0.10860470682382584\n",
      "Epoch 147: train loss: 0.10702168941497803\n",
      "Epoch 148: train loss: 0.10548324882984161\n",
      "Epoch 149: train loss: 0.10398364067077637\n",
      "Epoch 150: train loss: 0.10252298414707184\n",
      "Epoch 151: train loss: 0.10110177844762802\n",
      "Epoch 152: train loss: 0.09971591830253601\n",
      "Epoch 153: train loss: 0.09836433827877045\n",
      "Epoch 154: train loss: 0.09704650193452835\n",
      "Epoch 155: train loss: 0.09576670080423355\n",
      "Epoch 156: train loss: 0.09451793879270554\n",
      "Epoch 157: train loss: 0.09329934418201447\n",
      "Epoch 158: train loss: 0.09210996329784393\n",
      "Epoch 159: train loss: 0.0909489244222641\n",
      "Epoch 160: train loss: 0.08981539309024811\n",
      "Epoch 161: train loss: 0.0887085571885109\n",
      "Epoch 162: train loss: 0.0876276046037674\n",
      "Epoch 163: train loss: 0.08657176047563553\n",
      "Epoch 164: train loss: 0.08554025739431381\n",
      "Epoch 165: train loss: 0.08453239500522614\n",
      "Epoch 166: train loss: 0.08354832231998444\n",
      "Epoch 167: train loss: 0.0825878381729126\n",
      "Epoch 168: train loss: 0.0816487967967987\n",
      "Epoch 169: train loss: 0.08073059469461441\n",
      "Epoch 170: train loss: 0.07983255386352539\n",
      "Epoch 171: train loss: 0.07895412296056747\n",
      "Epoch 172: train loss: 0.0780947208404541\n",
      "Epoch 173: train loss: 0.0772537812590599\n",
      "Epoch 174: train loss: 0.07643076777458191\n",
      "Epoch 175: train loss: 0.07562515884637833\n",
      "Epoch 176: train loss: 0.07483645528554916\n",
      "Epoch 177: train loss: 0.07406418025493622\n",
      "Epoch 178: train loss: 0.07330784946680069\n",
      "Epoch 179: train loss: 0.07256699353456497\n",
      "Epoch 180: train loss: 0.07184119522571564\n",
      "Epoch 181: train loss: 0.07113002985715866\n",
      "Epoch 182: train loss: 0.07043322175741196\n",
      "Epoch 183: train loss: 0.06975150853395462\n",
      "Epoch 184: train loss: 0.06908552348613739\n",
      "Epoch 185: train loss: 0.06843243539333344\n",
      "Epoch 186: train loss: 0.0677918791770935\n",
      "Epoch 187: train loss: 0.06716354191303253\n",
      "Epoch 188: train loss: 0.06654706597328186\n",
      "Epoch 189: train loss: 0.06594214588403702\n",
      "Epoch 190: train loss: 0.06534846127033234\n",
      "Epoch 191: train loss: 0.06476573646068573\n",
      "Epoch 192: train loss: 0.06419365853071213\n",
      "Epoch 193: train loss: 0.06363195925951004\n",
      "Epoch 194: train loss: 0.06308035552501678\n",
      "Epoch 195: train loss: 0.06253860890865326\n",
      "Epoch 196: train loss: 0.062006451189517975\n",
      "Epoch 197: train loss: 0.06148364022374153\n",
      "Epoch 198: train loss: 0.06096993759274483\n",
      "Epoch 199: train loss: 0.060465119779109955\n",
      "Epoch 200: train loss: 0.05996895581483841\n",
      "Epoch 201: train loss: 0.059481244534254074\n",
      "Epoch 202: train loss: 0.05900173634290695\n",
      "Epoch 203: train loss: 0.058530282229185104\n",
      "Epoch 204: train loss: 0.0580669641494751\n",
      "Epoch 205: train loss: 0.057611845433712006\n",
      "Epoch 206: train loss: 0.05716415122151375\n",
      "Epoch 207: train loss: 0.05672402307391167\n",
      "Epoch 208: train loss: 0.05629200488328934\n",
      "Epoch 209: train loss: 0.055867940187454224\n",
      "Epoch 210: train loss: 0.055450551211833954\n",
      "Epoch 211: train loss: 0.055039674043655396\n",
      "Epoch 212: train loss: 0.05463547259569168\n",
      "Epoch 213: train loss: 0.05423783138394356\n",
      "Epoch 214: train loss: 0.053846221417188644\n",
      "Epoch 215: train loss: 0.05346054956316948\n",
      "Epoch 216: train loss: 0.05308065935969353\n",
      "Epoch 217: train loss: 0.052706412971019745\n",
      "Epoch 218: train loss: 0.05233769491314888\n",
      "Epoch 219: train loss: 0.05197439715266228\n",
      "Epoch 220: train loss: 0.05161638185381889\n",
      "Epoch 221: train loss: 0.05126354098320007\n",
      "Epoch 222: train loss: 0.050915785133838654\n",
      "Epoch 223: train loss: 0.050572969019412994\n",
      "Epoch 224: train loss: 0.05023501068353653\n",
      "Epoch 225: train loss: 0.0499018058180809\n",
      "Epoch 226: train loss: 0.04957326501607895\n",
      "Epoch 227: train loss: 0.04924926161766052\n",
      "Epoch 228: train loss: 0.048929713666439056\n",
      "Epoch 229: train loss: 0.048614561557769775\n",
      "Epoch 230: train loss: 0.048303671181201935\n",
      "Epoch 231: train loss: 0.04799697920680046\n",
      "Epoch 232: train loss: 0.04769439250230789\n",
      "Epoch 233: train loss: 0.047395844012498856\n",
      "Epoch 234: train loss: 0.04710124060511589\n",
      "Epoch 235: train loss: 0.04681050032377243\n",
      "Epoch 236: train loss: 0.0465235598385334\n",
      "Epoch 237: train loss: 0.046240344643592834\n",
      "Epoch 238: train loss: 0.04596076160669327\n",
      "Epoch 239: train loss: 0.04568475857377052\n",
      "Epoch 240: train loss: 0.045412272214889526\n",
      "Epoch 241: train loss: 0.04514322802424431\n",
      "Epoch 242: train loss: 0.0448775514960289\n",
      "Epoch 243: train loss: 0.04461519792675972\n",
      "Epoch 244: train loss: 0.0443560853600502\n",
      "Epoch 245: train loss: 0.04410015791654587\n",
      "Epoch 246: train loss: 0.04384741932153702\n",
      "Epoch 247: train loss: 0.043597880750894547\n",
      "Epoch 248: train loss: 0.04335135594010353\n",
      "Epoch 249: train loss: 0.043107770383358\n",
      "Epoch 250: train loss: 0.04286710172891617\n",
      "Epoch 251: train loss: 0.04262929409742355\n",
      "Epoch 252: train loss: 0.04239429160952568\n",
      "Epoch 253: train loss: 0.04216202348470688\n",
      "Epoch 254: train loss: 0.04193248227238655\n",
      "Epoch 255: train loss: 0.04170558229088783\n",
      "Epoch 256: train loss: 0.04148130863904953\n",
      "Epoch 257: train loss: 0.04125959798693657\n",
      "Epoch 258: train loss: 0.041040390729904175\n",
      "Epoch 259: train loss: 0.04082367569208145\n",
      "Epoch 260: train loss: 0.04060939699411392\n",
      "Epoch 261: train loss: 0.040397513657808304\n",
      "Epoch 262: train loss: 0.04018798843026161\n",
      "Epoch 263: train loss: 0.03998076170682907\n",
      "Epoch 264: train loss: 0.039775993674993515\n",
      "Epoch 265: train loss: 0.03957348316907883\n",
      "Epoch 266: train loss: 0.039373159408569336\n",
      "Epoch 267: train loss: 0.03917499631643295\n",
      "Epoch 268: train loss: 0.03897896036505699\n",
      "Epoch 269: train loss: 0.038785021752119064\n",
      "Epoch 270: train loss: 0.038593120872974396\n",
      "Epoch 271: train loss: 0.03840326890349388\n",
      "Epoch 272: train loss: 0.03821539133787155\n",
      "Epoch 273: train loss: 0.03802948072552681\n",
      "Epoch 274: train loss: 0.03784549981355667\n",
      "Epoch 275: train loss: 0.03766342252492905\n",
      "Epoch 276: train loss: 0.03748321533203125\n",
      "Epoch 277: train loss: 0.037304848432540894\n",
      "Epoch 278: train loss: 0.03712828457355499\n",
      "Epoch 279: train loss: 0.036953505128622055\n",
      "Epoch 280: train loss: 0.03678048774600029\n",
      "Epoch 281: train loss: 0.0366092212498188\n",
      "Epoch 282: train loss: 0.03643963113427162\n",
      "Epoch 283: train loss: 0.03627173975110054\n",
      "Epoch 284: train loss: 0.03610548377037048\n",
      "Epoch 285: train loss: 0.03594086319208145\n",
      "Epoch 286: train loss: 0.03577785938978195\n",
      "Epoch 287: train loss: 0.035616423934698105\n",
      "Epoch 288: train loss: 0.035456642508506775\n",
      "Epoch 289: train loss: 0.03529861941933632\n",
      "Epoch 290: train loss: 0.0351422056555748\n",
      "Epoch 291: train loss: 0.034987300634384155\n",
      "Epoch 292: train loss: 0.034833841025829315\n",
      "Epoch 293: train loss: 0.03468182682991028\n",
      "Epoch 294: train loss: 0.03453122451901436\n",
      "Epoch 295: train loss: 0.034382037818431854\n",
      "Epoch 296: train loss: 0.034234218299388885\n",
      "Epoch 297: train loss: 0.03408776596188545\n",
      "Epoch 298: train loss: 0.033942658454179764\n",
      "Epoch 299: train loss: 0.03379886597394943\n",
      "Epoch 300: train loss: 0.03365638852119446\n",
      "Epoch 301: train loss: 0.03351517766714096\n",
      "Epoch 302: train loss: 0.03337525203824043\n",
      "Epoch 303: train loss: 0.03323657065629959\n",
      "Epoch 304: train loss: 0.033099133521318436\n",
      "Epoch 305: train loss: 0.03296290710568428\n",
      "Epoch 306: train loss: 0.03282787650823593\n",
      "Epoch 307: train loss: 0.03269404545426369\n",
      "Epoch 308: train loss: 0.032561369240283966\n",
      "Epoch 309: train loss: 0.03242986649274826\n",
      "Epoch 310: train loss: 0.03229949623346329\n",
      "Epoch 311: train loss: 0.03217024356126785\n",
      "Epoch 312: train loss: 0.03204210847616196\n",
      "Epoch 313: train loss: 0.03191506117582321\n",
      "Epoch 314: train loss: 0.03178909793496132\n",
      "Epoch 315: train loss: 0.03166421502828598\n",
      "Epoch 316: train loss: 0.03154037892818451\n",
      "Epoch 317: train loss: 0.031417571008205414\n",
      "Epoch 318: train loss: 0.03129580244421959\n",
      "Epoch 319: train loss: 0.031175043433904648\n",
      "Epoch 320: train loss: 0.031055282801389694\n",
      "Epoch 321: train loss: 0.030936509370803833\n",
      "Epoch 322: train loss: 0.030818719416856766\n",
      "Epoch 323: train loss: 0.030701879411935806\n",
      "Epoch 324: train loss: 0.03058600053191185\n",
      "Epoch 325: train loss: 0.030471062287688255\n",
      "Epoch 326: train loss: 0.030357051640748978\n",
      "Epoch 327: train loss: 0.03024395927786827\n",
      "Epoch 328: train loss: 0.030131768435239792\n",
      "Epoch 329: train loss: 0.030020469799637794\n",
      "Epoch 330: train loss: 0.029910067096352577\n",
      "Epoch 331: train loss: 0.0298005398362875\n",
      "Epoch 332: train loss: 0.02969186007976532\n",
      "Epoch 333: train loss: 0.029584044590592384\n",
      "Epoch 334: train loss: 0.029477069154381752\n",
      "Epoch 335: train loss: 0.029370928183197975\n",
      "Epoch 336: train loss: 0.029265597462654114\n",
      "Epoch 337: train loss: 0.02916109189391136\n",
      "Epoch 338: train loss: 0.029057389125227928\n",
      "Epoch 339: train loss: 0.02895447611808777\n",
      "Epoch 340: train loss: 0.028852343559265137\n",
      "Epoch 341: train loss: 0.02875099703669548\n",
      "Epoch 342: train loss: 0.028650406748056412\n",
      "Epoch 343: train loss: 0.028550589457154274\n",
      "Epoch 344: train loss: 0.028451502323150635\n",
      "Epoch 345: train loss: 0.02835315838456154\n",
      "Epoch 346: train loss: 0.028255466371774673\n",
      "Epoch 347: train loss: 0.0281585194170475\n",
      "Epoch 348: train loss: 0.02806227281689644\n",
      "Epoch 349: train loss: 0.02796674333512783\n",
      "Epoch 350: train loss: 0.027871916070580482\n",
      "Epoch 351: train loss: 0.027777761220932007\n",
      "Epoch 352: train loss: 0.02768431231379509\n",
      "Epoch 353: train loss: 0.027591535821557045\n",
      "Epoch 354: train loss: 0.02749944105744362\n",
      "Epoch 355: train loss: 0.027407994493842125\n",
      "Epoch 356: train loss: 0.02731720544397831\n",
      "Epoch 357: train loss: 0.02722708322107792\n",
      "Epoch 358: train loss: 0.027137571945786476\n",
      "Epoch 359: train loss: 0.027048712596297264\n",
      "Epoch 360: train loss: 0.026960482820868492\n",
      "Epoch 361: train loss: 0.026872877031564713\n",
      "Epoch 362: train loss: 0.026785871013998985\n",
      "Epoch 363: train loss: 0.02669949270784855\n",
      "Epoch 364: train loss: 0.026613708585500717\n",
      "Epoch 365: train loss: 0.026528513059020042\n",
      "Epoch 366: train loss: 0.026443924754858017\n",
      "Epoch 367: train loss: 0.02635997161269188\n",
      "Epoch 368: train loss: 0.026276592165231705\n",
      "Epoch 369: train loss: 0.026193782687187195\n",
      "Epoch 370: train loss: 0.026111532002687454\n",
      "Epoch 371: train loss: 0.02602984569966793\n",
      "Epoch 372: train loss: 0.025948693975806236\n",
      "Epoch 373: train loss: 0.025868112221360207\n",
      "Epoch 374: train loss: 0.02578805945813656\n",
      "Epoch 375: train loss: 0.025708535686135292\n",
      "Epoch 376: train loss: 0.0256295595318079\n",
      "Epoch 377: train loss: 0.025551090016961098\n",
      "Epoch 378: train loss: 0.02547314763069153\n",
      "Epoch 379: train loss: 0.025395721197128296\n",
      "Epoch 380: train loss: 0.025318792089819908\n",
      "Epoch 381: train loss: 0.02524236962199211\n",
      "Epoch 382: train loss: 0.025166455656290054\n",
      "Epoch 383: train loss: 0.025091037154197693\n",
      "Epoch 384: train loss: 0.025016093626618385\n",
      "Epoch 385: train loss: 0.024941643700003624\n",
      "Epoch 386: train loss: 0.02486765757203102\n",
      "Epoch 387: train loss: 0.024794165045022964\n",
      "Epoch 388: train loss: 0.02472112514078617\n",
      "Epoch 389: train loss: 0.024648567661643028\n",
      "Epoch 390: train loss: 0.0245764572173357\n",
      "Epoch 391: train loss: 0.02450481429696083\n",
      "Epoch 392: train loss: 0.024433612823486328\n",
      "Epoch 393: train loss: 0.024362852796912193\n",
      "Epoch 394: train loss: 0.02429255284368992\n",
      "Epoch 395: train loss: 0.02422267571091652\n",
      "Epoch 396: train loss: 0.024153243750333786\n",
      "Epoch 397: train loss: 0.024084249511361122\n",
      "Epoch 398: train loss: 0.024015668779611588\n",
      "Epoch 399: train loss: 0.023947520181536674\n",
      "Epoch 400: train loss: 0.023879792541265488\n",
      "Epoch 401: train loss: 0.02381247654557228\n",
      "Epoch 402: train loss: 0.023745562881231308\n",
      "Epoch 403: train loss: 0.02367905154824257\n",
      "Epoch 404: train loss: 0.023612961173057556\n",
      "Epoch 405: train loss: 0.02354726381599903\n",
      "Epoch 406: train loss: 0.023481953889131546\n",
      "Epoch 407: train loss: 0.02341703698039055\n",
      "Epoch 408: train loss: 0.023352527990937233\n",
      "Epoch 409: train loss: 0.023288395255804062\n",
      "Epoch 410: train loss: 0.02322463132441044\n",
      "Epoch 411: train loss: 0.02316124178469181\n",
      "Epoch 412: train loss: 0.023098239675164223\n",
      "Epoch 413: train loss: 0.02303561568260193\n",
      "Epoch 414: train loss: 0.022973347455263138\n",
      "Epoch 415: train loss: 0.022911440581083298\n",
      "Epoch 416: train loss: 0.0228499174118042\n",
      "Epoch 417: train loss: 0.022788722068071365\n",
      "Epoch 418: train loss: 0.022727902978658676\n",
      "Epoch 419: train loss: 0.022667426615953445\n",
      "Epoch 420: train loss: 0.022607292979955673\n",
      "Epoch 421: train loss: 0.0225475262850523\n",
      "Epoch 422: train loss: 0.02248808741569519\n",
      "Epoch 423: train loss: 0.02242899313569069\n",
      "Epoch 424: train loss: 0.02237023413181305\n",
      "Epoch 425: train loss: 0.022311795502901077\n",
      "Epoch 426: train loss: 0.022253701463341713\n",
      "Epoch 427: train loss: 0.022195929661393166\n",
      "Epoch 428: train loss: 0.02213849313557148\n",
      "Epoch 429: train loss: 0.02208137884736061\n",
      "Epoch 430: train loss: 0.02202456071972847\n",
      "Epoch 431: train loss: 0.021968085318803787\n",
      "Epoch 432: train loss: 0.02191191166639328\n",
      "Epoch 433: train loss: 0.021856049075722694\n",
      "Epoch 434: train loss: 0.021800503134727478\n",
      "Epoch 435: train loss: 0.02174525521695614\n",
      "Epoch 436: train loss: 0.021690305322408676\n",
      "Epoch 437: train loss: 0.02163567766547203\n",
      "Epoch 438: train loss: 0.021581336855888367\n",
      "Epoch 439: train loss: 0.02152729406952858\n",
      "Epoch 440: train loss: 0.021473539993166924\n",
      "Epoch 441: train loss: 0.02142007276415825\n",
      "Epoch 442: train loss: 0.021366897970438004\n",
      "Epoch 443: train loss: 0.021314013749361038\n",
      "Epoch 444: train loss: 0.021261412650346756\n",
      "Epoch 445: train loss: 0.02120908908545971\n",
      "Epoch 446: train loss: 0.021157048642635345\n",
      "Epoch 447: train loss: 0.021105285733938217\n",
      "Epoch 448: train loss: 0.021053798496723175\n",
      "Epoch 449: train loss: 0.02100258506834507\n",
      "Epoch 450: train loss: 0.020951636135578156\n",
      "Epoch 451: train loss: 0.020900961011648178\n",
      "Epoch 452: train loss: 0.020850559696555138\n",
      "Epoch 453: train loss: 0.020800406113266945\n",
      "Epoch 454: train loss: 0.02075052075088024\n",
      "Epoch 455: train loss: 0.020700877532362938\n",
      "Epoch 456: train loss: 0.020651521161198616\n",
      "Epoch 457: train loss: 0.020602408796548843\n",
      "Epoch 458: train loss: 0.020553547888994217\n",
      "Epoch 459: train loss: 0.020504940301179886\n",
      "Epoch 460: train loss: 0.02045661397278309\n",
      "Epoch 461: train loss: 0.02040853351354599\n",
      "Epoch 462: train loss: 0.020360711961984634\n",
      "Epoch 463: train loss: 0.020313097164034843\n",
      "Epoch 464: train loss: 0.020265750586986542\n",
      "Epoch 465: train loss: 0.020218640565872192\n",
      "Epoch 466: train loss: 0.0201717559248209\n",
      "Epoch 467: train loss: 0.020125122740864754\n",
      "Epoch 468: train loss: 0.020078707486391068\n",
      "Epoch 469: train loss: 0.02003253623843193\n",
      "Epoch 470: train loss: 0.01998659409582615\n",
      "Epoch 471: train loss: 0.019940881058573723\n",
      "Epoch 472: train loss: 0.019895393401384354\n",
      "Epoch 473: train loss: 0.019850140437483788\n",
      "Epoch 474: train loss: 0.019805099815130234\n",
      "Epoch 475: train loss: 0.01976027712225914\n",
      "Epoch 476: train loss: 0.019715677946805954\n",
      "Epoch 477: train loss: 0.01967131346464157\n",
      "Epoch 478: train loss: 0.019627150148153305\n",
      "Epoch 479: train loss: 0.0195832010358572\n",
      "Epoch 480: train loss: 0.019539466127753258\n",
      "Epoch 481: train loss: 0.019495941698551178\n",
      "Epoch 482: train loss: 0.019452642649412155\n",
      "Epoch 483: train loss: 0.019409554079174995\n",
      "Epoch 484: train loss: 0.019366657361388206\n",
      "Epoch 485: train loss: 0.01932396925985813\n",
      "Epoch 486: train loss: 0.019281480461359024\n",
      "Epoch 487: train loss: 0.01923919841647148\n",
      "Epoch 488: train loss: 0.019197121262550354\n",
      "Epoch 489: train loss: 0.01915525086224079\n",
      "Epoch 490: train loss: 0.01911356672644615\n",
      "Epoch 491: train loss: 0.019072087481617928\n",
      "Epoch 492: train loss: 0.019030796363949776\n",
      "Epoch 493: train loss: 0.018989702686667442\n",
      "Epoch 494: train loss: 0.018948813900351524\n",
      "Epoch 495: train loss: 0.018908092752099037\n",
      "Epoch 496: train loss: 0.018867582082748413\n",
      "Epoch 497: train loss: 0.018827244639396667\n",
      "Epoch 498: train loss: 0.01878710463643074\n",
      "Epoch 499: train loss: 0.018747154623270035\n",
      "Epoch 500: train loss: 0.018707383424043655\n",
      "Epoch 501: train loss: 0.018667787313461304\n",
      "Epoch 502: train loss: 0.018628377467393875\n",
      "Epoch 503: train loss: 0.01858915202319622\n",
      "Epoch 504: train loss: 0.018550097942352295\n",
      "Epoch 505: train loss: 0.01851123757660389\n",
      "Epoch 506: train loss: 0.018472541123628616\n",
      "Epoch 507: train loss: 0.018434032797813416\n",
      "Epoch 508: train loss: 0.018395688384771347\n",
      "Epoch 509: train loss: 0.018357519060373306\n",
      "Epoch 510: train loss: 0.018319519236683846\n",
      "Epoch 511: train loss: 0.01828170195221901\n",
      "Epoch 512: train loss: 0.018244046717882156\n",
      "Epoch 513: train loss: 0.01820654235780239\n",
      "Epoch 514: train loss: 0.0181692223995924\n",
      "Epoch 515: train loss: 0.018132075667381287\n",
      "Epoch 516: train loss: 0.018095090985298157\n",
      "Epoch 517: train loss: 0.018058260902762413\n",
      "Epoch 518: train loss: 0.018021615222096443\n",
      "Epoch 519: train loss: 0.01798509620130062\n",
      "Epoch 520: train loss: 0.01794876530766487\n",
      "Epoch 521: train loss: 0.01791257970035076\n",
      "Epoch 522: train loss: 0.017876561731100082\n",
      "Epoch 523: train loss: 0.01784069836139679\n",
      "Epoch 524: train loss: 0.017804985865950584\n",
      "Epoch 525: train loss: 0.01776944473385811\n",
      "Epoch 526: train loss: 0.017734047025442123\n",
      "Epoch 527: train loss: 0.01769879460334778\n",
      "Epoch 528: train loss: 0.017663709819316864\n",
      "Epoch 529: train loss: 0.01762876659631729\n",
      "Epoch 530: train loss: 0.017593977972865105\n",
      "Epoch 531: train loss: 0.01755933091044426\n",
      "Epoch 532: train loss: 0.017524845898151398\n",
      "Epoch 533: train loss: 0.01749049872159958\n",
      "Epoch 534: train loss: 0.01745629496872425\n",
      "Epoch 535: train loss: 0.017422229051589966\n",
      "Epoch 536: train loss: 0.017388327047228813\n",
      "Epoch 537: train loss: 0.0173545703291893\n",
      "Epoch 538: train loss: 0.01732093095779419\n",
      "Epoch 539: train loss: 0.01728745549917221\n",
      "Epoch 540: train loss: 0.017254123464226723\n",
      "Epoch 541: train loss: 0.017220910638570786\n",
      "Epoch 542: train loss: 0.017187856137752533\n",
      "Epoch 543: train loss: 0.017154918983578682\n",
      "Epoch 544: train loss: 0.01712212711572647\n",
      "Epoch 545: train loss: 0.0170894842594862\n",
      "Epoch 546: train loss: 0.01705695502460003\n",
      "Epoch 547: train loss: 0.017024587839841843\n",
      "Epoch 548: train loss: 0.016992323100566864\n",
      "Epoch 549: train loss: 0.01696021482348442\n",
      "Epoch 550: train loss: 0.016928264871239662\n",
      "Epoch 551: train loss: 0.0168964471668005\n",
      "Epoch 552: train loss: 0.016864748671650887\n",
      "Epoch 553: train loss: 0.016833176836371422\n",
      "Epoch 554: train loss: 0.0168017465621233\n",
      "Epoch 555: train loss: 0.016770435497164726\n",
      "Epoch 556: train loss: 0.016739249229431152\n",
      "Epoch 557: train loss: 0.016708189621567726\n",
      "Epoch 558: train loss: 0.016677251085639\n",
      "Epoch 559: train loss: 0.016646461561322212\n",
      "Epoch 560: train loss: 0.016615767031908035\n",
      "Epoch 561: train loss: 0.016585206612944603\n",
      "Epoch 562: train loss: 0.01655476540327072\n",
      "Epoch 563: train loss: 0.016524454578757286\n",
      "Epoch 564: train loss: 0.01649424619972706\n",
      "Epoch 565: train loss: 0.01646416261792183\n",
      "Epoch 566: train loss: 0.016434213146567345\n",
      "Epoch 567: train loss: 0.01640436425805092\n",
      "Epoch 568: train loss: 0.016374647617340088\n",
      "Epoch 569: train loss: 0.01634504273533821\n",
      "Epoch 570: train loss: 0.016315549612045288\n",
      "Epoch 571: train loss: 0.016286185011267662\n",
      "Epoch 572: train loss: 0.016256917268037796\n",
      "Epoch 573: train loss: 0.01622777245938778\n",
      "Epoch 574: train loss: 0.016198735684156418\n",
      "Epoch 575: train loss: 0.016169816255569458\n",
      "Epoch 576: train loss: 0.01614101231098175\n",
      "Epoch 577: train loss: 0.016112323850393295\n",
      "Epoch 578: train loss: 0.01608373038470745\n",
      "Epoch 579: train loss: 0.016055257990956306\n",
      "Epoch 580: train loss: 0.01602688618004322\n",
      "Epoch 581: train loss: 0.015998627990484238\n",
      "Epoch 582: train loss: 0.015970487147569656\n",
      "Epoch 583: train loss: 0.015942439436912537\n",
      "Epoch 584: train loss: 0.01591450348496437\n",
      "Epoch 585: train loss: 0.015886684879660606\n",
      "Epoch 586: train loss: 0.015858963131904602\n",
      "Epoch 587: train loss: 0.015831347554922104\n",
      "Epoch 588: train loss: 0.015803826972842216\n",
      "Epoch 589: train loss: 0.015776414424180984\n",
      "Epoch 590: train loss: 0.015749117359519005\n",
      "Epoch 591: train loss: 0.015721911564469337\n",
      "Epoch 592: train loss: 0.015694808214902878\n",
      "Epoch 593: train loss: 0.015667807310819626\n",
      "Epoch 594: train loss: 0.015640921890735626\n",
      "Epoch 595: train loss: 0.015614109113812447\n",
      "Epoch 596: train loss: 0.01558740995824337\n",
      "Epoch 597: train loss: 0.015560813248157501\n",
      "Epoch 598: train loss: 0.015534314326941967\n",
      "Epoch 599: train loss: 0.015507909469306469\n",
      "Epoch 600: train loss: 0.01548159122467041\n",
      "Epoch 601: train loss: 0.015455384738743305\n",
      "Epoch 602: train loss: 0.015429268591105938\n",
      "Epoch 603: train loss: 0.015403257682919502\n",
      "Epoch 604: train loss: 0.015377333387732506\n",
      "Epoch 605: train loss: 0.015351520851254463\n",
      "Epoch 606: train loss: 0.015325787477195263\n",
      "Epoch 607: train loss: 0.015300137922167778\n",
      "Epoch 608: train loss: 0.015274596400558949\n",
      "Epoch 609: train loss: 0.01524914801120758\n",
      "Epoch 610: train loss: 0.015223774127662182\n",
      "Epoch 611: train loss: 0.015198498964309692\n",
      "Epoch 612: train loss: 0.01517332810908556\n",
      "Epoch 613: train loss: 0.015148229897022247\n",
      "Epoch 614: train loss: 0.015123237855732441\n",
      "Epoch 615: train loss: 0.015098325908184052\n",
      "Epoch 616: train loss: 0.015073506161570549\n",
      "Epoch 617: train loss: 0.015048772096633911\n",
      "Epoch 618: train loss: 0.015024138614535332\n",
      "Epoch 619: train loss: 0.014999574050307274\n",
      "Epoch 620: train loss: 0.014975106343626976\n",
      "Epoch 621: train loss: 0.01495071779936552\n",
      "Epoch 622: train loss: 0.014926423318684101\n",
      "Epoch 623: train loss: 0.014902213588356972\n",
      "Epoch 624: train loss: 0.01487809233367443\n",
      "Epoch 625: train loss: 0.014854048378765583\n",
      "Epoch 626: train loss: 0.014830085448920727\n",
      "Epoch 627: train loss: 0.014806220307946205\n",
      "Epoch 628: train loss: 0.014782434329390526\n",
      "Epoch 629: train loss: 0.014758719131350517\n",
      "Epoch 630: train loss: 0.01473509706556797\n",
      "Epoch 631: train loss: 0.014711554162204266\n",
      "Epoch 632: train loss: 0.014688089489936829\n",
      "Epoch 633: train loss: 0.014664724469184875\n",
      "Epoch 634: train loss: 0.01464142370969057\n",
      "Epoch 635: train loss: 0.014618203043937683\n",
      "Epoch 636: train loss: 0.014595059677958488\n",
      "Epoch 637: train loss: 0.014572001993656158\n",
      "Epoch 638: train loss: 0.014549019746482372\n",
      "Epoch 639: train loss: 0.014526116661727428\n",
      "Epoch 640: train loss: 0.01450330764055252\n",
      "Epoch 641: train loss: 0.01448055636137724\n",
      "Epoch 642: train loss: 0.01445787213742733\n",
      "Epoch 643: train loss: 0.014435294084250927\n",
      "Epoch 644: train loss: 0.014412770979106426\n",
      "Epoch 645: train loss: 0.014390328899025917\n",
      "Epoch 646: train loss: 0.014367975294589996\n",
      "Epoch 647: train loss: 0.01434569526463747\n",
      "Epoch 648: train loss: 0.01432347483932972\n",
      "Epoch 649: train loss: 0.014301339164376259\n",
      "Epoch 650: train loss: 0.014279264025390148\n",
      "Epoch 651: train loss: 0.014257279224693775\n",
      "Epoch 652: train loss: 0.014235367998480797\n",
      "Epoch 653: train loss: 0.01421352755278349\n",
      "Epoch 654: train loss: 0.014191756024956703\n",
      "Epoch 655: train loss: 0.014170053415000439\n",
      "Epoch 656: train loss: 0.014148431830108166\n",
      "Epoch 657: train loss: 0.014126865193247795\n",
      "Epoch 658: train loss: 0.014105403795838356\n",
      "Epoch 659: train loss: 0.01408397126942873\n",
      "Epoch 660: train loss: 0.014062640257179737\n",
      "Epoch 661: train loss: 0.0140413548797369\n",
      "Epoch 662: train loss: 0.014020152390003204\n",
      "Epoch 663: train loss: 0.01399902068078518\n",
      "Epoch 664: train loss: 0.013977967202663422\n",
      "Epoch 665: train loss: 0.013956964015960693\n",
      "Epoch 666: train loss: 0.01393603254109621\n",
      "Epoch 667: train loss: 0.01391516625881195\n",
      "Epoch 668: train loss: 0.013894389383494854\n",
      "Epoch 669: train loss: 0.013873648829758167\n",
      "Epoch 670: train loss: 0.013853001408278942\n",
      "Epoch 671: train loss: 0.013832408003509045\n",
      "Epoch 672: train loss: 0.013811880722641945\n",
      "Epoch 673: train loss: 0.01379142701625824\n",
      "Epoch 674: train loss: 0.013771037571132183\n",
      "Epoch 675: train loss: 0.013750704936683178\n",
      "Epoch 676: train loss: 0.013730439357459545\n",
      "Epoch 677: train loss: 0.013710247352719307\n",
      "Epoch 678: train loss: 0.013690100982785225\n",
      "Epoch 679: train loss: 0.013670037500560284\n",
      "Epoch 680: train loss: 0.013650037348270416\n",
      "Epoch 681: train loss: 0.013630084693431854\n",
      "Epoch 682: train loss: 0.013610206544399261\n",
      "Epoch 683: train loss: 0.013590390793979168\n",
      "Epoch 684: train loss: 0.013570638373494148\n",
      "Epoch 685: train loss: 0.013550939969718456\n",
      "Epoch 686: train loss: 0.013531324453651905\n",
      "Epoch 687: train loss: 0.01351175457239151\n",
      "Epoch 688: train loss: 0.01349224429577589\n",
      "Epoch 689: train loss: 0.013472792692482471\n",
      "Epoch 690: train loss: 0.013453418388962746\n",
      "Epoch 691: train loss: 0.013434072025120258\n",
      "Epoch 692: train loss: 0.013414820656180382\n",
      "Epoch 693: train loss: 0.013395613059401512\n",
      "Epoch 694: train loss: 0.013376469723880291\n",
      "Epoch 695: train loss: 0.013357383199036121\n",
      "Epoch 696: train loss: 0.013338351622223854\n",
      "Epoch 697: train loss: 0.013319380581378937\n",
      "Epoch 698: train loss: 0.013300472870469093\n",
      "Epoch 699: train loss: 0.01328161358833313\n",
      "Epoch 700: train loss: 0.013262835331261158\n",
      "Epoch 701: train loss: 0.0132440822198987\n",
      "Epoch 702: train loss: 0.01322540920227766\n",
      "Epoch 703: train loss: 0.013206792064011097\n",
      "Epoch 704: train loss: 0.013188213109970093\n",
      "Epoch 705: train loss: 0.01316971518099308\n",
      "Epoch 706: train loss: 0.013151256367564201\n",
      "Epoch 707: train loss: 0.013132862746715546\n",
      "Epoch 708: train loss: 0.013114526867866516\n",
      "Epoch 709: train loss: 0.013096238486468792\n",
      "Epoch 710: train loss: 0.013077996671199799\n",
      "Epoch 711: train loss: 0.013059821911156178\n",
      "Epoch 712: train loss: 0.013041689991950989\n",
      "Epoch 713: train loss: 0.013023634441196918\n",
      "Epoch 714: train loss: 0.013005624525249004\n",
      "Epoch 715: train loss: 0.01298766303807497\n",
      "Epoch 716: train loss: 0.012969749979674816\n",
      "Epoch 717: train loss: 0.012951898388564587\n",
      "Epoch 718: train loss: 0.012934108264744282\n",
      "Epoch 719: train loss: 0.01291634887456894\n",
      "Epoch 720: train loss: 0.012898663990199566\n",
      "Epoch 721: train loss: 0.012881023809313774\n",
      "Epoch 722: train loss: 0.012863430194556713\n",
      "Epoch 723: train loss: 0.0128459008410573\n",
      "Epoch 724: train loss: 0.012828417122364044\n",
      "Epoch 725: train loss: 0.012810979969799519\n",
      "Epoch 726: train loss: 0.012793594971299171\n",
      "Epoch 727: train loss: 0.012776263058185577\n",
      "Epoch 728: train loss: 0.012758979573845863\n",
      "Epoch 729: train loss: 0.012741747312247753\n",
      "Epoch 730: train loss: 0.01272456906735897\n",
      "Epoch 731: train loss: 0.012707442045211792\n",
      "Epoch 732: train loss: 0.012690357863903046\n",
      "Epoch 733: train loss: 0.01267333049327135\n",
      "Epoch 734: train loss: 0.012656350620090961\n",
      "Epoch 735: train loss: 0.012639420107007027\n",
      "Epoch 736: train loss: 0.0126225296407938\n",
      "Epoch 737: train loss: 0.012605704367160797\n",
      "Epoch 738: train loss: 0.012588913552463055\n",
      "Epoch 739: train loss: 0.012572166509926319\n",
      "Epoch 740: train loss: 0.012555490247905254\n",
      "Epoch 741: train loss: 0.012538844719529152\n",
      "Epoch 742: train loss: 0.012522255070507526\n",
      "Epoch 743: train loss: 0.012505699880421162\n",
      "Epoch 744: train loss: 0.012489208951592445\n",
      "Epoch 745: train loss: 0.012472751550376415\n",
      "Epoch 746: train loss: 0.012456347234547138\n",
      "Epoch 747: train loss: 0.012439980171620846\n",
      "Epoch 748: train loss: 0.012423673644661903\n",
      "Epoch 749: train loss: 0.01240741927176714\n",
      "Epoch 750: train loss: 0.012391187250614166\n",
      "Epoch 751: train loss: 0.012375012040138245\n",
      "Epoch 752: train loss: 0.012358885258436203\n",
      "Epoch 753: train loss: 0.012342802248895168\n",
      "Epoch 754: train loss: 0.012326767668128014\n",
      "Epoch 755: train loss: 0.012310781516134739\n",
      "Epoch 756: train loss: 0.012294815853238106\n",
      "Epoch 757: train loss: 0.01227891631424427\n",
      "Epoch 758: train loss: 0.012263065204024315\n",
      "Epoch 759: train loss: 0.012247246690094471\n",
      "Epoch 760: train loss: 0.012231473810970783\n",
      "Epoch 761: train loss: 0.012215742841362953\n",
      "Epoch 762: train loss: 0.012200066819787025\n",
      "Epoch 763: train loss: 0.012184416875243187\n",
      "Epoch 764: train loss: 0.012168822810053825\n",
      "Epoch 765: train loss: 0.012153263203799725\n",
      "Epoch 766: train loss: 0.012137760408222675\n",
      "Epoch 767: train loss: 0.012122290208935738\n",
      "Epoch 768: train loss: 0.012106862850487232\n",
      "Epoch 769: train loss: 0.012091483920812607\n",
      "Epoch 770: train loss: 0.012076137587428093\n",
      "Epoch 771: train loss: 0.012060833163559437\n",
      "Epoch 772: train loss: 0.012045583687722683\n",
      "Epoch 773: train loss: 0.012030369602143764\n",
      "Epoch 774: train loss: 0.012015185318887234\n",
      "Epoch 775: train loss: 0.012000062502920628\n",
      "Epoch 776: train loss: 0.011984961107373238\n",
      "Epoch 777: train loss: 0.0119699165225029\n",
      "Epoch 778: train loss: 0.011954905465245247\n",
      "Epoch 779: train loss: 0.01193993166089058\n",
      "Epoch 780: train loss: 0.011925006285309792\n",
      "Epoch 781: train loss: 0.011910121887922287\n",
      "Epoch 782: train loss: 0.011895271018147469\n",
      "Epoch 783: train loss: 0.011880457401275635\n",
      "Epoch 784: train loss: 0.01186568383127451\n",
      "Epoch 785: train loss: 0.011850974522531033\n",
      "Epoch 786: train loss: 0.0118362782523036\n",
      "Epoch 787: train loss: 0.011821618303656578\n",
      "Epoch 788: train loss: 0.011807017028331757\n",
      "Epoch 789: train loss: 0.011792441830039024\n",
      "Epoch 790: train loss: 0.01177790854126215\n",
      "Epoch 791: train loss: 0.01176341064274311\n",
      "Epoch 792: train loss: 0.011748941615223885\n",
      "Epoch 793: train loss: 0.011734528467059135\n",
      "Epoch 794: train loss: 0.011720148846507072\n",
      "Epoch 795: train loss: 0.011705800890922546\n",
      "Epoch 796: train loss: 0.011691506020724773\n",
      "Epoch 797: train loss: 0.01167722512036562\n",
      "Epoch 798: train loss: 0.011663006618618965\n",
      "Epoch 799: train loss: 0.011648818850517273\n",
      "Epoch 800: train loss: 0.01163465715944767\n",
      "Epoch 801: train loss: 0.01162053644657135\n",
      "Epoch 802: train loss: 0.011606455780565739\n",
      "Epoch 803: train loss: 0.011592419818043709\n",
      "Epoch 804: train loss: 0.011578401550650597\n",
      "Epoch 805: train loss: 0.01156442891806364\n",
      "Epoch 806: train loss: 0.01155050564557314\n",
      "Epoch 807: train loss: 0.011536605656147003\n",
      "Epoch 808: train loss: 0.011522742919623852\n",
      "Epoch 809: train loss: 0.01150891650468111\n",
      "Epoch 810: train loss: 0.011495119892060757\n",
      "Epoch 811: train loss: 0.011481355875730515\n",
      "Epoch 812: train loss: 0.011467646807432175\n",
      "Epoch 813: train loss: 0.011453955434262753\n",
      "Epoch 814: train loss: 0.011440308764576912\n",
      "Epoch 815: train loss: 0.01142669003456831\n",
      "Epoch 816: train loss: 0.011413109488785267\n",
      "Epoch 817: train loss: 0.011399565264582634\n",
      "Epoch 818: train loss: 0.011386053636670113\n",
      "Epoch 819: train loss: 0.011372582986950874\n",
      "Epoch 820: train loss: 0.011359135620296001\n",
      "Epoch 821: train loss: 0.011345721781253815\n",
      "Epoch 822: train loss: 0.011332360096275806\n",
      "Epoch 823: train loss: 0.01131901890039444\n",
      "Epoch 824: train loss: 0.011305722407996655\n",
      "Epoch 825: train loss: 0.0112924724817276\n",
      "Epoch 826: train loss: 0.01127925980836153\n",
      "Epoch 827: train loss: 0.011266093701124191\n",
      "Epoch 828: train loss: 0.011252946220338345\n",
      "Epoch 829: train loss: 0.011239834129810333\n",
      "Epoch 830: train loss: 0.011226758360862732\n",
      "Epoch 831: train loss: 0.01121370680630207\n",
      "Epoch 832: train loss: 0.011200702749192715\n",
      "Epoch 833: train loss: 0.01118771918118\n",
      "Epoch 834: train loss: 0.011174771003425121\n",
      "Epoch 835: train loss: 0.011161858215928078\n",
      "Epoch 836: train loss: 0.011148973368108273\n",
      "Epoch 837: train loss: 0.01113611925393343\n",
      "Epoch 838: train loss: 0.011123291216790676\n",
      "Epoch 839: train loss: 0.01111051719635725\n",
      "Epoch 840: train loss: 0.011097760871052742\n",
      "Epoch 841: train loss: 0.011085030622780323\n",
      "Epoch 842: train loss: 0.011072327382862568\n",
      "Epoch 843: train loss: 0.011059674434363842\n",
      "Epoch 844: train loss: 0.011047045700252056\n",
      "Epoch 845: train loss: 0.011034435592591763\n",
      "Epoch 846: train loss: 0.011021875776350498\n",
      "Epoch 847: train loss: 0.011009329929947853\n",
      "Epoch 848: train loss: 0.010996825993061066\n",
      "Epoch 849: train loss: 0.010984346270561218\n",
      "Epoch 850: train loss: 0.01097190659493208\n",
      "Epoch 851: train loss: 0.010959471575915813\n",
      "Epoch 852: train loss: 0.010947104543447495\n",
      "Epoch 853: train loss: 0.010934743098914623\n",
      "Epoch 854: train loss: 0.010922417975962162\n",
      "Epoch 855: train loss: 0.010910113342106342\n",
      "Epoch 856: train loss: 0.010897845029830933\n",
      "Epoch 857: train loss: 0.010885605588555336\n",
      "Epoch 858: train loss: 0.01087339036166668\n",
      "Epoch 859: train loss: 0.01086122915148735\n",
      "Epoch 860: train loss: 0.01084907352924347\n",
      "Epoch 861: train loss: 0.010836958885192871\n",
      "Epoch 862: train loss: 0.010824853554368019\n",
      "Epoch 863: train loss: 0.010812804102897644\n",
      "Epoch 864: train loss: 0.010800769552588463\n",
      "Epoch 865: train loss: 0.0107887526974082\n",
      "Epoch 866: train loss: 0.010776784271001816\n",
      "Epoch 867: train loss: 0.010764842852950096\n",
      "Epoch 868: train loss: 0.010752922855317593\n",
      "Epoch 869: train loss: 0.010741024278104305\n",
      "Epoch 870: train loss: 0.010729167610406876\n",
      "Epoch 871: train loss: 0.010717329569160938\n",
      "Epoch 872: train loss: 0.010705518536269665\n",
      "Epoch 873: train loss: 0.01069374568760395\n",
      "Epoch 874: train loss: 0.010681992396712303\n",
      "Epoch 875: train loss: 0.010670268908143044\n",
      "Epoch 876: train loss: 0.010658588260412216\n",
      "Epoch 877: train loss: 0.010646906681358814\n",
      "Epoch 878: train loss: 0.010635264217853546\n",
      "Epoch 879: train loss: 0.01062365435063839\n",
      "Epoch 880: train loss: 0.010612064972519875\n",
      "Epoch 881: train loss: 0.010600507259368896\n",
      "Epoch 882: train loss: 0.010588973760604858\n",
      "Epoch 883: train loss: 0.010577468201518059\n",
      "Epoch 884: train loss: 0.010565990582108498\n",
      "Epoch 885: train loss: 0.010554535314440727\n",
      "Epoch 886: train loss: 0.010543117299675941\n",
      "Epoch 887: train loss: 0.010531709529459476\n",
      "Epoch 888: train loss: 0.010520342737436295\n",
      "Epoch 889: train loss: 0.010508988983929157\n",
      "Epoch 890: train loss: 0.010497679933905602\n",
      "Epoch 891: train loss: 0.010486389510333538\n",
      "Epoch 892: train loss: 0.010475121438503265\n",
      "Epoch 893: train loss: 0.010463882237672806\n",
      "Epoch 894: train loss: 0.010452650487422943\n",
      "Epoch 895: train loss: 0.010441471822559834\n",
      "Epoch 896: train loss: 0.010430297814309597\n",
      "Epoch 897: train loss: 0.010419170372188091\n",
      "Epoch 898: train loss: 0.010408056899905205\n",
      "Epoch 899: train loss: 0.010396975092589855\n",
      "Epoch 900: train loss: 0.010385902598500252\n",
      "Epoch 901: train loss: 0.010374864563345909\n",
      "Epoch 902: train loss: 0.01036385353654623\n",
      "Epoch 903: train loss: 0.010352866724133492\n",
      "Epoch 904: train loss: 0.010341906920075417\n",
      "Epoch 905: train loss: 0.01033095270395279\n",
      "Epoch 906: train loss: 0.010320037603378296\n",
      "Epoch 907: train loss: 0.010309163480997086\n",
      "Epoch 908: train loss: 0.010298303328454494\n",
      "Epoch 909: train loss: 0.010287456214427948\n",
      "Epoch 910: train loss: 0.010276639834046364\n",
      "Epoch 911: train loss: 0.010265854187309742\n",
      "Epoch 912: train loss: 0.010255081579089165\n",
      "Epoch 913: train loss: 0.010244346223771572\n",
      "Epoch 914: train loss: 0.010233626700937748\n",
      "Epoch 915: train loss: 0.010222934186458588\n",
      "Epoch 916: train loss: 0.010212262161076069\n",
      "Epoch 917: train loss: 0.010201624594628811\n",
      "Epoch 918: train loss: 0.01019099447876215\n",
      "Epoch 919: train loss: 0.010180389508605003\n",
      "Epoch 920: train loss: 0.010169824585318565\n",
      "Epoch 921: train loss: 0.010159267112612724\n",
      "Epoch 922: train loss: 0.010148739442229271\n",
      "Epoch 923: train loss: 0.010138234123587608\n",
      "Epoch 924: train loss: 0.01012775581330061\n",
      "Epoch 925: train loss: 0.01011728961020708\n",
      "Epoch 926: train loss: 0.010106866247951984\n",
      "Epoch 927: train loss: 0.010096436366438866\n",
      "Epoch 928: train loss: 0.010086051188409328\n",
      "Epoch 929: train loss: 0.010075693018734455\n",
      "Epoch 930: train loss: 0.010065339505672455\n",
      "Epoch 931: train loss: 0.010055014863610268\n",
      "Epoch 932: train loss: 0.010044721886515617\n",
      "Epoch 933: train loss: 0.010034442879259586\n",
      "Epoch 934: train loss: 0.010024188086390495\n",
      "Epoch 935: train loss: 0.010013963095843792\n",
      "Epoch 936: train loss: 0.010003764182329178\n",
      "Epoch 937: train loss: 0.009993567131459713\n",
      "Epoch 938: train loss: 0.009983396157622337\n",
      "Epoch 939: train loss: 0.009973262436687946\n",
      "Epoch 940: train loss: 0.009963145479559898\n",
      "Epoch 941: train loss: 0.00995304249227047\n",
      "Epoch 942: train loss: 0.009942964650690556\n",
      "Epoch 943: train loss: 0.009932905435562134\n",
      "Epoch 944: train loss: 0.009922871366143227\n",
      "Epoch 945: train loss: 0.009912863373756409\n",
      "Epoch 946: train loss: 0.009902875870466232\n",
      "Epoch 947: train loss: 0.00989290326833725\n",
      "Epoch 948: train loss: 0.009882966987788677\n",
      "Epoch 949: train loss: 0.009873032569885254\n",
      "Epoch 950: train loss: 0.009863128885626793\n",
      "Epoch 951: train loss: 0.009853252209722996\n",
      "Epoch 952: train loss: 0.009843381121754646\n",
      "Epoch 953: train loss: 0.009833544492721558\n",
      "Epoch 954: train loss: 0.009823723696172237\n",
      "Epoch 955: train loss: 0.00981392152607441\n",
      "Epoch 956: train loss: 0.009804137982428074\n",
      "Epoch 957: train loss: 0.00979438703507185\n",
      "Epoch 958: train loss: 0.009784653782844543\n",
      "Epoch 959: train loss: 0.009774940088391304\n",
      "Epoch 960: train loss: 0.00976523570716381\n",
      "Epoch 961: train loss: 0.009755567647516727\n",
      "Epoch 962: train loss: 0.009745917282998562\n",
      "Epoch 963: train loss: 0.009736279956996441\n",
      "Epoch 964: train loss: 0.00972666684538126\n",
      "Epoch 965: train loss: 0.009717075154185295\n",
      "Epoch 966: train loss: 0.009707503020763397\n",
      "Epoch 967: train loss: 0.00969794113188982\n",
      "Epoch 968: train loss: 0.009688416495919228\n",
      "Epoch 969: train loss: 0.009678887203335762\n",
      "Epoch 970: train loss: 0.009669402614235878\n",
      "Epoch 971: train loss: 0.009659925475716591\n",
      "Epoch 972: train loss: 0.009650475345551968\n",
      "Epoch 973: train loss: 0.009641041979193687\n",
      "Epoch 974: train loss: 0.009631626307964325\n",
      "Epoch 975: train loss: 0.009622232057154179\n",
      "Epoch 976: train loss: 0.00961285550147295\n",
      "Epoch 977: train loss: 0.009603498503565788\n",
      "Epoch 978: train loss: 0.009594166651368141\n",
      "Epoch 979: train loss: 0.009584849700331688\n",
      "Epoch 980: train loss: 0.009575550444424152\n",
      "Epoch 981: train loss: 0.009566274471580982\n",
      "Epoch 982: train loss: 0.009557013399899006\n",
      "Epoch 983: train loss: 0.009547775611281395\n",
      "Epoch 984: train loss: 0.009538559243083\n",
      "Epoch 985: train loss: 0.009529352188110352\n",
      "Epoch 986: train loss: 0.009520168416202068\n",
      "Epoch 987: train loss: 0.009511005133390427\n",
      "Epoch 988: train loss: 0.009501861408352852\n",
      "Epoch 989: train loss: 0.009492730721831322\n",
      "Epoch 990: train loss: 0.009483618661761284\n",
      "Epoch 991: train loss: 0.009474532678723335\n",
      "Epoch 992: train loss: 0.009465465322136879\n",
      "Epoch 993: train loss: 0.009456408210098743\n",
      "Epoch 994: train loss: 0.009447372518479824\n",
      "Epoch 995: train loss: 0.009438350796699524\n",
      "Epoch 996: train loss: 0.00942936260253191\n",
      "Epoch 997: train loss: 0.009420384652912617\n",
      "Epoch 998: train loss: 0.00941142626106739\n",
      "Epoch 999: train loss: 0.009402472525835037\n",
      "Epoch 1000: train loss: 0.00939355418086052\n",
      "Epoch 1001: train loss: 0.009384646080434322\n",
      "Epoch 1002: train loss: 0.009375757537782192\n",
      "Epoch 1003: train loss: 0.009366892278194427\n",
      "Epoch 1004: train loss: 0.009358031675219536\n",
      "Epoch 1005: train loss: 0.009349199011921883\n",
      "Epoch 1006: train loss: 0.009340392425656319\n",
      "Epoch 1007: train loss: 0.00933158490806818\n",
      "Epoch 1008: train loss: 0.009322796016931534\n",
      "Epoch 1009: train loss: 0.00931403785943985\n",
      "Epoch 1010: train loss: 0.009305287152528763\n",
      "Epoch 1011: train loss: 0.009296553209424019\n",
      "Epoch 1012: train loss: 0.009287843480706215\n",
      "Epoch 1013: train loss: 0.009279145859181881\n",
      "Epoch 1014: train loss: 0.009270468726754189\n",
      "Epoch 1015: train loss: 0.009261811152100563\n",
      "Epoch 1016: train loss: 0.009253175929188728\n",
      "Epoch 1017: train loss: 0.009244547225534916\n",
      "Epoch 1018: train loss: 0.009235934354364872\n",
      "Epoch 1019: train loss: 0.009227343834936619\n",
      "Epoch 1020: train loss: 0.009218772873282433\n",
      "Epoch 1021: train loss: 0.009210210293531418\n",
      "Epoch 1022: train loss: 0.009201671928167343\n",
      "Epoch 1023: train loss: 0.009193139150738716\n",
      "Epoch 1024: train loss: 0.009184638038277626\n",
      "Epoch 1025: train loss: 0.009176145307719707\n",
      "Epoch 1026: train loss: 0.009167680516839027\n",
      "Epoch 1027: train loss: 0.009159212931990623\n",
      "Epoch 1028: train loss: 0.009150775149464607\n",
      "Epoch 1029: train loss: 0.009142347611486912\n",
      "Epoch 1030: train loss: 0.00913394708186388\n",
      "Epoch 1031: train loss: 0.009125555865466595\n",
      "Epoch 1032: train loss: 0.00911717489361763\n",
      "Epoch 1033: train loss: 0.009108814410865307\n",
      "Epoch 1034: train loss: 0.00910047348588705\n",
      "Epoch 1035: train loss: 0.009092148393392563\n",
      "Epoch 1036: train loss: 0.009083833545446396\n",
      "Epoch 1037: train loss: 0.009075550362467766\n",
      "Epoch 1038: train loss: 0.009067265316843987\n",
      "Epoch 1039: train loss: 0.0090589988976717\n",
      "Epoch 1040: train loss: 0.009050754830241203\n",
      "Epoch 1041: train loss: 0.009042533114552498\n",
      "Epoch 1042: train loss: 0.009034311398863792\n",
      "Epoch 1043: train loss: 0.009026123210787773\n",
      "Epoch 1044: train loss: 0.009017940610647202\n",
      "Epoch 1045: train loss: 0.009009760804474354\n",
      "Epoch 1046: train loss: 0.009001615457236767\n",
      "Epoch 1047: train loss: 0.008993485011160374\n",
      "Epoch 1048: train loss: 0.008985370397567749\n",
      "Epoch 1049: train loss: 0.008977272547781467\n",
      "Epoch 1050: train loss: 0.008969174697995186\n",
      "Epoch 1051: train loss: 0.008961102925240993\n",
      "Epoch 1052: train loss: 0.0089530348777771\n",
      "Epoch 1053: train loss: 0.00894499383866787\n",
      "Epoch 1054: train loss: 0.008936970494687557\n",
      "Epoch 1055: train loss: 0.008928957395255566\n",
      "Epoch 1056: train loss: 0.008920962922275066\n",
      "Epoch 1057: train loss: 0.008912974037230015\n",
      "Epoch 1058: train loss: 0.008905017748475075\n",
      "Epoch 1059: train loss: 0.00889706052839756\n",
      "Epoch 1060: train loss: 0.00888911820948124\n",
      "Epoch 1061: train loss: 0.008881209418177605\n",
      "Epoch 1062: train loss: 0.008873306214809418\n",
      "Epoch 1063: train loss: 0.008865399286150932\n",
      "Epoch 1064: train loss: 0.008857538923621178\n",
      "Epoch 1065: train loss: 0.008849674835801125\n",
      "Epoch 1066: train loss: 0.008841830305755138\n",
      "Epoch 1067: train loss: 0.008834006264805794\n",
      "Epoch 1068: train loss: 0.008826185017824173\n",
      "Epoch 1069: train loss: 0.008818378672003746\n",
      "Epoch 1070: train loss: 0.008810596540570259\n",
      "Epoch 1071: train loss: 0.008802825585007668\n",
      "Epoch 1072: train loss: 0.008795060217380524\n",
      "Epoch 1073: train loss: 0.008787326514720917\n",
      "Epoch 1074: train loss: 0.008779594674706459\n",
      "Epoch 1075: train loss: 0.008771883323788643\n",
      "Epoch 1076: train loss: 0.008764175698161125\n",
      "Epoch 1077: train loss: 0.008756490424275398\n",
      "Epoch 1078: train loss: 0.00874883122742176\n",
      "Epoch 1079: train loss: 0.008741163648664951\n",
      "Epoch 1080: train loss: 0.008733522146940231\n",
      "Epoch 1081: train loss: 0.008725902065634727\n",
      "Epoch 1082: train loss: 0.00871827732771635\n",
      "Epoch 1083: train loss: 0.008710676804184914\n",
      "Epoch 1084: train loss: 0.008703097701072693\n",
      "Epoch 1085: train loss: 0.008695506490767002\n",
      "Epoch 1086: train loss: 0.008687963709235191\n",
      "Epoch 1087: train loss: 0.008680421859025955\n",
      "Epoch 1088: train loss: 0.008672894909977913\n",
      "Epoch 1089: train loss: 0.008665370754897594\n",
      "Epoch 1090: train loss: 0.008657876402139664\n",
      "Epoch 1091: train loss: 0.008650370873510838\n",
      "Epoch 1092: train loss: 0.008642894215881824\n",
      "Epoch 1093: train loss: 0.00863543152809143\n",
      "Epoch 1094: train loss: 0.00862798374146223\n",
      "Epoch 1095: train loss: 0.008620557375252247\n",
      "Epoch 1096: train loss: 0.008613131940364838\n",
      "Epoch 1097: train loss: 0.008605718612670898\n",
      "Epoch 1098: train loss: 0.0085983257740736\n",
      "Epoch 1099: train loss: 0.008590957149863243\n",
      "Epoch 1100: train loss: 0.008583578281104565\n",
      "Epoch 1101: train loss: 0.00857624039053917\n",
      "Epoch 1102: train loss: 0.008568909019231796\n",
      "Epoch 1103: train loss: 0.008561598137021065\n",
      "Epoch 1104: train loss: 0.008554285392165184\n",
      "Epoch 1105: train loss: 0.008546995930373669\n",
      "Epoch 1106: train loss: 0.008539717644453049\n",
      "Epoch 1107: train loss: 0.008532458916306496\n",
      "Epoch 1108: train loss: 0.008525199256837368\n",
      "Epoch 1109: train loss: 0.008517961949110031\n",
      "Epoch 1110: train loss: 0.008510743267834187\n",
      "Epoch 1111: train loss: 0.00850352831184864\n",
      "Epoch 1112: train loss: 0.008496327325701714\n",
      "Epoch 1113: train loss: 0.00848914310336113\n",
      "Epoch 1114: train loss: 0.008481968194246292\n",
      "Epoch 1115: train loss: 0.008474810048937798\n",
      "Epoch 1116: train loss: 0.008467656560242176\n",
      "Epoch 1117: train loss: 0.008460518904030323\n",
      "Epoch 1118: train loss: 0.008453388698399067\n",
      "Epoch 1119: train loss: 0.008446285501122475\n",
      "Epoch 1120: train loss: 0.008439183235168457\n",
      "Epoch 1121: train loss: 0.008432099595665932\n",
      "Epoch 1122: train loss: 0.008425028063356876\n",
      "Epoch 1123: train loss: 0.008417973294854164\n",
      "Epoch 1124: train loss: 0.008410918526351452\n",
      "Epoch 1125: train loss: 0.008403880521655083\n",
      "Epoch 1126: train loss: 0.008396861143410206\n",
      "Epoch 1127: train loss: 0.008389855735003948\n",
      "Epoch 1128: train loss: 0.008382854983210564\n",
      "Epoch 1129: train loss: 0.008375867269933224\n",
      "Epoch 1130: train loss: 0.00836888700723648\n",
      "Epoch 1131: train loss: 0.008361928164958954\n",
      "Epoch 1132: train loss: 0.008354974910616875\n",
      "Epoch 1133: train loss: 0.008348040282726288\n",
      "Epoch 1134: train loss: 0.008341116830706596\n",
      "Epoch 1135: train loss: 0.008334212005138397\n",
      "Epoch 1136: train loss: 0.0083273034542799\n",
      "Epoch 1137: train loss: 0.008320415392518044\n",
      "Epoch 1138: train loss: 0.008313539437949657\n",
      "Epoch 1139: train loss: 0.008306673727929592\n",
      "Epoch 1140: train loss: 0.008299825713038445\n",
      "Epoch 1141: train loss: 0.008292978629469872\n",
      "Epoch 1142: train loss: 0.00828615017235279\n",
      "Epoch 1143: train loss: 0.008279340341687202\n",
      "Epoch 1144: train loss: 0.00827252957969904\n",
      "Epoch 1145: train loss: 0.00826574768871069\n",
      "Epoch 1146: train loss: 0.00825895369052887\n",
      "Epoch 1147: train loss: 0.00825219601392746\n",
      "Epoch 1148: train loss: 0.008245422504842281\n",
      "Epoch 1149: train loss: 0.008238677866756916\n",
      "Epoch 1150: train loss: 0.008231953717768192\n",
      "Epoch 1151: train loss: 0.008225223049521446\n",
      "Epoch 1152: train loss: 0.008218517526984215\n",
      "Epoch 1153: train loss: 0.008211801759898663\n",
      "Epoch 1154: train loss: 0.008205113001167774\n",
      "Epoch 1155: train loss: 0.00819843914359808\n",
      "Epoch 1156: train loss: 0.008191774599254131\n",
      "Epoch 1157: train loss: 0.008185126818716526\n",
      "Epoch 1158: train loss: 0.008178474381566048\n",
      "Epoch 1159: train loss: 0.008171849884092808\n",
      "Epoch 1160: train loss: 0.008165223523974419\n",
      "Epoch 1161: train loss: 0.008158618584275246\n",
      "Epoch 1162: train loss: 0.008152016438543797\n",
      "Epoch 1163: train loss: 0.00814543291926384\n",
      "Epoch 1164: train loss: 0.0081388670951128\n",
      "Epoch 1165: train loss: 0.00813229475170374\n",
      "Epoch 1166: train loss: 0.008125734515488148\n",
      "Epoch 1167: train loss: 0.008119186386466026\n",
      "Epoch 1168: train loss: 0.008112667128443718\n",
      "Epoch 1169: train loss: 0.008106137625873089\n",
      "Epoch 1170: train loss: 0.008099636994302273\n",
      "Epoch 1171: train loss: 0.008093138225376606\n",
      "Epoch 1172: train loss: 0.00808664970099926\n",
      "Epoch 1173: train loss: 0.008080175146460533\n",
      "Epoch 1174: train loss: 0.008073711767792702\n",
      "Epoch 1175: train loss: 0.00806726049631834\n",
      "Epoch 1176: train loss: 0.00806081760674715\n",
      "Epoch 1177: train loss: 0.008054377511143684\n",
      "Epoch 1178: train loss: 0.008047946728765965\n",
      "Epoch 1179: train loss: 0.00804154947400093\n",
      "Epoch 1180: train loss: 0.0080351447686553\n",
      "Epoch 1181: train loss: 0.008028754033148289\n",
      "Epoch 1182: train loss: 0.008022377267479897\n",
      "Epoch 1183: train loss: 0.008016014471650124\n",
      "Epoch 1184: train loss: 0.008009659126400948\n",
      "Epoch 1185: train loss: 0.008003313094377518\n",
      "Epoch 1186: train loss: 0.007996965199708939\n",
      "Epoch 1187: train loss: 0.0079906415194273\n",
      "Epoch 1188: train loss: 0.007984327152371407\n",
      "Epoch 1189: train loss: 0.007978013716638088\n",
      "Epoch 1190: train loss: 0.007971716113388538\n",
      "Epoch 1191: train loss: 0.007965447381138802\n",
      "Epoch 1192: train loss: 0.00795916747301817\n",
      "Epoch 1193: train loss: 0.007952900603413582\n",
      "Epoch 1194: train loss: 0.007946651428937912\n",
      "Epoch 1195: train loss: 0.007940405048429966\n",
      "Epoch 1196: train loss: 0.007934166118502617\n",
      "Epoch 1197: train loss: 0.007927948608994484\n",
      "Epoch 1198: train loss: 0.007921739481389523\n",
      "Epoch 1199: train loss: 0.007915543392300606\n",
      "Epoch 1200: train loss: 0.007909351028501987\n",
      "Epoch 1201: train loss: 0.007903171703219414\n",
      "Epoch 1202: train loss: 0.00789699424058199\n",
      "Epoch 1203: train loss: 0.007890831679105759\n",
      "Epoch 1204: train loss: 0.007884691469371319\n",
      "Epoch 1205: train loss: 0.007878533564507961\n",
      "Epoch 1206: train loss: 0.007872410118579865\n",
      "Epoch 1207: train loss: 0.007866287603974342\n",
      "Epoch 1208: train loss: 0.007860173471271992\n",
      "Epoch 1209: train loss: 0.007854068651795387\n",
      "Epoch 1210: train loss: 0.00784799549728632\n",
      "Epoch 1211: train loss: 0.007841899991035461\n",
      "Epoch 1212: train loss: 0.007835835218429565\n",
      "Epoch 1213: train loss: 0.007829764857888222\n",
      "Epoch 1214: train loss: 0.007823724299669266\n",
      "Epoch 1215: train loss: 0.007817666046321392\n",
      "Epoch 1216: train loss: 0.007811637129634619\n",
      "Epoch 1217: train loss: 0.007805613335222006\n",
      "Epoch 1218: train loss: 0.00779960211366415\n",
      "Epoch 1219: train loss: 0.007793606724590063\n",
      "Epoch 1220: train loss: 0.007787617389112711\n",
      "Epoch 1221: train loss: 0.007781622465699911\n",
      "Epoch 1222: train loss: 0.007775659207254648\n",
      "Epoch 1223: train loss: 0.007769688032567501\n",
      "Epoch 1224: train loss: 0.007763730827718973\n",
      "Epoch 1225: train loss: 0.007757788989692926\n",
      "Epoch 1226: train loss: 0.007751853205263615\n",
      "Epoch 1227: train loss: 0.007745932787656784\n",
      "Epoch 1228: train loss: 0.007740015629678965\n",
      "Epoch 1229: train loss: 0.0077341035939753056\n",
      "Epoch 1230: train loss: 0.007728198077529669\n",
      "Epoch 1231: train loss: 0.007722318172454834\n",
      "Epoch 1232: train loss: 0.007716433610767126\n",
      "Epoch 1233: train loss: 0.007710568606853485\n",
      "Epoch 1234: train loss: 0.007704713381826878\n",
      "Epoch 1235: train loss: 0.007698860950767994\n",
      "Epoch 1236: train loss: 0.00769300851970911\n",
      "Epoch 1237: train loss: 0.007687167730182409\n",
      "Epoch 1238: train loss: 0.007681360002607107\n",
      "Epoch 1239: train loss: 0.007675538770854473\n",
      "Epoch 1240: train loss: 0.007669734302908182\n",
      "Epoch 1241: train loss: 0.007663937751203775\n",
      "Epoch 1242: train loss: 0.0076581696048378944\n",
      "Epoch 1243: train loss: 0.0076523697935044765\n",
      "Epoch 1244: train loss: 0.007646614219993353\n",
      "Epoch 1245: train loss: 0.007640860043466091\n",
      "Epoch 1246: train loss: 0.007635106332600117\n",
      "Epoch 1247: train loss: 0.00762936333194375\n",
      "Epoch 1248: train loss: 0.007623637560755014\n",
      "Epoch 1249: train loss: 0.0076178936287760735\n",
      "Epoch 1250: train loss: 0.007612189743667841\n",
      "Epoch 1251: train loss: 0.007606495171785355\n",
      "Epoch 1252: train loss: 0.007600793149322271\n",
      "Epoch 1253: train loss: 0.007595112081617117\n",
      "Epoch 1254: train loss: 0.0075894249603152275\n",
      "Epoch 1255: train loss: 0.007583764847368002\n",
      "Epoch 1256: train loss: 0.007578106131404638\n",
      "Epoch 1257: train loss: 0.007572450675070286\n",
      "Epoch 1258: train loss: 0.007566811051219702\n",
      "Epoch 1259: train loss: 0.007561174221336842\n",
      "Epoch 1260: train loss: 0.007555562071502209\n",
      "Epoch 1261: train loss: 0.007549927569925785\n",
      "Epoch 1262: train loss: 0.007544317748397589\n",
      "Epoch 1263: train loss: 0.007538719568401575\n",
      "Epoch 1264: train loss: 0.007533133029937744\n",
      "Epoch 1265: train loss: 0.007527553476393223\n",
      "Epoch 1266: train loss: 0.007521984167397022\n",
      "Epoch 1267: train loss: 0.007516422308981419\n",
      "Epoch 1268: train loss: 0.007510860916227102\n",
      "Epoch 1269: train loss: 0.007505316287279129\n",
      "Epoch 1270: train loss: 0.0074997819028794765\n",
      "Epoch 1271: train loss: 0.007494261953979731\n",
      "Epoch 1272: train loss: 0.007488728500902653\n",
      "Epoch 1273: train loss: 0.007483216933906078\n",
      "Epoch 1274: train loss: 0.007477708160877228\n",
      "Epoch 1275: train loss: 0.0074722180142998695\n",
      "Epoch 1276: train loss: 0.007466716226190329\n",
      "Epoch 1277: train loss: 0.007461243309080601\n",
      "Epoch 1278: train loss: 0.007455779705196619\n",
      "Epoch 1279: train loss: 0.007450310979038477\n",
      "Epoch 1280: train loss: 0.007444857619702816\n",
      "Epoch 1281: train loss: 0.007439414504915476\n",
      "Epoch 1282: train loss: 0.007433964870870113\n",
      "Epoch 1283: train loss: 0.0074285417795181274\n",
      "Epoch 1284: train loss: 0.007423128932714462\n",
      "Epoch 1285: train loss: 0.007417710032314062\n",
      "Epoch 1286: train loss: 0.007412313017994165\n",
      "Epoch 1287: train loss: 0.007406918797641993\n",
      "Epoch 1288: train loss: 0.007401522248983383\n",
      "Epoch 1289: train loss: 0.007396140601485968\n",
      "Epoch 1290: train loss: 0.007390767335891724\n",
      "Epoch 1291: train loss: 0.007385420147329569\n",
      "Epoch 1292: train loss: 0.0073800524696707726\n",
      "Epoch 1293: train loss: 0.007374715991318226\n",
      "Epoch 1294: train loss: 0.007369375322014093\n",
      "Epoch 1295: train loss: 0.007364042103290558\n",
      "Epoch 1296: train loss: 0.007358710281550884\n",
      "Epoch 1297: train loss: 0.007353408727794886\n",
      "Epoch 1298: train loss: 0.007348109036684036\n",
      "Epoch 1299: train loss: 0.007342788390815258\n",
      "Epoch 1300: train loss: 0.007337503135204315\n",
      "Epoch 1301: train loss: 0.007332212291657925\n",
      "Epoch 1302: train loss: 0.007326950319111347\n",
      "Epoch 1303: train loss: 0.007321679499000311\n",
      "Epoch 1304: train loss: 0.007316422648727894\n",
      "Epoch 1305: train loss: 0.007311174180358648\n",
      "Epoch 1306: train loss: 0.007305932231247425\n",
      "Epoch 1307: train loss: 0.0073006851598620415\n",
      "Epoch 1308: train loss: 0.007295466028153896\n",
      "Epoch 1309: train loss: 0.007290243171155453\n",
      "Epoch 1310: train loss: 0.007285033818334341\n",
      "Epoch 1311: train loss: 0.00727982958778739\n",
      "Epoch 1312: train loss: 0.007274632807821035\n",
      "Epoch 1313: train loss: 0.007269441150128841\n",
      "Epoch 1314: train loss: 0.007264255080372095\n",
      "Epoch 1315: train loss: 0.0072590745985507965\n",
      "Epoch 1316: train loss: 0.007253921590745449\n",
      "Epoch 1317: train loss: 0.007248769048601389\n",
      "Epoch 1318: train loss: 0.007243603467941284\n",
      "Epoch 1319: train loss: 0.00723846722394228\n",
      "Epoch 1320: train loss: 0.007233327720314264\n",
      "Epoch 1321: train loss: 0.0072282105684280396\n",
      "Epoch 1322: train loss: 0.007223081775009632\n",
      "Epoch 1323: train loss: 0.007217966020107269\n",
      "Epoch 1324: train loss: 0.007212857250124216\n",
      "Epoch 1325: train loss: 0.007207770831882954\n",
      "Epoch 1326: train loss: 0.007202683482319117\n",
      "Epoch 1327: train loss: 0.0071975914761424065\n",
      "Epoch 1328: train loss: 0.007192522287368774\n",
      "Epoch 1329: train loss: 0.007187449838966131\n",
      "Epoch 1330: train loss: 0.007182388566434383\n",
      "Epoch 1331: train loss: 0.007177345454692841\n",
      "Epoch 1332: train loss: 0.007172289304435253\n",
      "Epoch 1333: train loss: 0.007167255971580744\n",
      "Epoch 1334: train loss: 0.007162219379097223\n",
      "Epoch 1335: train loss: 0.007157192565500736\n",
      "Epoch 1336: train loss: 0.007152182515710592\n",
      "Epoch 1337: train loss: 0.007147177122533321\n",
      "Epoch 1338: train loss: 0.007142174988985062\n",
      "Epoch 1339: train loss: 0.007137179374694824\n",
      "Epoch 1340: train loss: 0.007132192142307758\n",
      "Epoch 1341: train loss: 0.007127216551452875\n",
      "Epoch 1342: train loss: 0.007122240960597992\n",
      "Epoch 1343: train loss: 0.007117274217307568\n",
      "Epoch 1344: train loss: 0.0071123093366622925\n",
      "Epoch 1345: train loss: 0.007107364479452372\n",
      "Epoch 1346: train loss: 0.0071024238131940365\n",
      "Epoch 1347: train loss: 0.007097491063177586\n",
      "Epoch 1348: train loss: 0.0070925503969192505\n",
      "Epoch 1349: train loss: 0.007087640464305878\n",
      "Epoch 1350: train loss: 0.0070827193558216095\n",
      "Epoch 1351: train loss: 0.007077815476804972\n",
      "Epoch 1352: train loss: 0.007072918117046356\n",
      "Epoch 1353: train loss: 0.007068017031997442\n",
      "Epoch 1354: train loss: 0.007063128985464573\n",
      "Epoch 1355: train loss: 0.007058258168399334\n",
      "Epoch 1356: train loss: 0.007053376641124487\n",
      "Epoch 1357: train loss: 0.007048512808978558\n",
      "Epoch 1358: train loss: 0.0070436568930745125\n",
      "Epoch 1359: train loss: 0.007038799114525318\n",
      "Epoch 1360: train loss: 0.007033959962427616\n",
      "Epoch 1361: train loss: 0.007029117550700903\n",
      "Epoch 1362: train loss: 0.007024289574474096\n",
      "Epoch 1363: train loss: 0.007019464857876301\n",
      "Epoch 1364: train loss: 0.007014644332230091\n",
      "Epoch 1365: train loss: 0.007009836379438639\n",
      "Epoch 1366: train loss: 0.007005028426647186\n",
      "Epoch 1367: train loss: 0.007000228855758905\n",
      "Epoch 1368: train loss: 0.006995444186031818\n",
      "Epoch 1369: train loss: 0.00699066836386919\n",
      "Epoch 1370: train loss: 0.006985888816416264\n",
      "Epoch 1371: train loss: 0.006981114391237497\n",
      "Epoch 1372: train loss: 0.006976352073252201\n",
      "Epoch 1373: train loss: 0.006971597671508789\n",
      "Epoch 1374: train loss: 0.006966844201087952\n",
      "Epoch 1375: train loss: 0.006962099578231573\n",
      "Epoch 1376: train loss: 0.006957364268600941\n",
      "Epoch 1377: train loss: 0.00695263734087348\n",
      "Epoch 1378: train loss: 0.006947922054678202\n",
      "Epoch 1379: train loss: 0.00694319698959589\n",
      "Epoch 1380: train loss: 0.0069384886883199215\n",
      "Epoch 1381: train loss: 0.006933786906301975\n",
      "Epoch 1382: train loss: 0.00692908838391304\n",
      "Epoch 1383: train loss: 0.006924404297024012\n",
      "Epoch 1384: train loss: 0.00691971555352211\n",
      "Epoch 1385: train loss: 0.006915051490068436\n",
      "Epoch 1386: train loss: 0.006910367868840694\n",
      "Epoch 1387: train loss: 0.0069057149812579155\n",
      "Epoch 1388: train loss: 0.006901049055159092\n",
      "Epoch 1389: train loss: 0.006896394304931164\n",
      "Epoch 1390: train loss: 0.006891751196235418\n",
      "Epoch 1391: train loss: 0.006887123920023441\n",
      "Epoch 1392: train loss: 0.006882490124553442\n",
      "Epoch 1393: train loss: 0.00687786191701889\n",
      "Epoch 1394: train loss: 0.006873256526887417\n",
      "Epoch 1395: train loss: 0.006868641823530197\n",
      "Epoch 1396: train loss: 0.006864032708108425\n",
      "Epoch 1397: train loss: 0.006859445013105869\n",
      "Epoch 1398: train loss: 0.006854837294667959\n",
      "Epoch 1399: train loss: 0.0068502528592944145\n",
      "Epoch 1400: train loss: 0.006845682859420776\n",
      "Epoch 1401: train loss: 0.006841103080660105\n",
      "Epoch 1402: train loss: 0.006836537271738052\n",
      "Epoch 1403: train loss: 0.0068319798447191715\n",
      "Epoch 1404: train loss: 0.006827420089393854\n",
      "Epoch 1405: train loss: 0.0068228780291974545\n",
      "Epoch 1406: train loss: 0.006818328984081745\n",
      "Epoch 1407: train loss: 0.0068137929774820805\n",
      "Epoch 1408: train loss: 0.00680927699431777\n",
      "Epoch 1409: train loss: 0.006804737262427807\n",
      "Epoch 1410: train loss: 0.006800227798521519\n",
      "Epoch 1411: train loss: 0.0067957257851958275\n",
      "Epoch 1412: train loss: 0.006791211664676666\n",
      "Epoch 1413: train loss: 0.006786715239286423\n",
      "Epoch 1414: train loss: 0.006782223470509052\n",
      "Epoch 1415: train loss: 0.006777741014957428\n",
      "Epoch 1416: train loss: 0.006773259490728378\n",
      "Epoch 1417: train loss: 0.0067687854170799255\n",
      "Epoch 1418: train loss: 0.006764323450624943\n",
      "Epoch 1419: train loss: 0.006759861018508673\n",
      "Epoch 1420: train loss: 0.006755406502634287\n",
      "Epoch 1421: train loss: 0.006750958506017923\n",
      "Epoch 1422: train loss: 0.00674651563167572\n",
      "Epoch 1423: train loss: 0.006742074154317379\n",
      "Epoch 1424: train loss: 0.0067376503720879555\n",
      "Epoch 1425: train loss: 0.006733216345310211\n",
      "Epoch 1426: train loss: 0.006728798151016235\n",
      "Epoch 1427: train loss: 0.0067243860103189945\n",
      "Epoch 1428: train loss: 0.006719990633428097\n",
      "Epoch 1429: train loss: 0.0067155808210372925\n",
      "Epoch 1430: train loss: 0.0067111896350979805\n",
      "Epoch 1431: train loss: 0.006706783082336187\n",
      "Epoch 1432: train loss: 0.006702403537929058\n",
      "Epoch 1433: train loss: 0.006698019802570343\n",
      "Epoch 1434: train loss: 0.006693660281598568\n",
      "Epoch 1435: train loss: 0.006689290050417185\n",
      "Epoch 1436: train loss: 0.006684935186058283\n",
      "Epoch 1437: train loss: 0.00668058916926384\n",
      "Epoch 1438: train loss: 0.006676240358501673\n",
      "Epoch 1439: train loss: 0.00667189946398139\n",
      "Epoch 1440: train loss: 0.006667565554380417\n",
      "Epoch 1441: train loss: 0.006663230247795582\n",
      "Epoch 1442: train loss: 0.006658904254436493\n",
      "Epoch 1443: train loss: 0.0066545819863677025\n",
      "Epoch 1444: train loss: 0.006650276482105255\n",
      "Epoch 1445: train loss: 0.006645973771810532\n",
      "Epoch 1446: train loss: 0.006641673389822245\n",
      "Epoch 1447: train loss: 0.006637366954237223\n",
      "Epoch 1448: train loss: 0.0066330814734101295\n",
      "Epoch 1449: train loss: 0.006628795061260462\n",
      "Epoch 1450: train loss: 0.006624521221965551\n",
      "Epoch 1451: train loss: 0.006620239466428757\n",
      "Epoch 1452: train loss: 0.00661597540602088\n",
      "Epoch 1453: train loss: 0.006611709948629141\n",
      "Epoch 1454: train loss: 0.00660746032372117\n",
      "Epoch 1455: train loss: 0.006603213958442211\n",
      "Epoch 1456: train loss: 0.006598972715437412\n",
      "Epoch 1457: train loss: 0.006594733335077763\n",
      "Epoch 1458: train loss: 0.006590497680008411\n",
      "Epoch 1459: train loss: 0.006586270872503519\n",
      "Epoch 1460: train loss: 0.006582041271030903\n",
      "Epoch 1461: train loss: 0.006577819585800171\n",
      "Epoch 1462: train loss: 0.006573612801730633\n",
      "Epoch 1463: train loss: 0.006569419987499714\n",
      "Epoch 1464: train loss: 0.006565218325704336\n",
      "Epoch 1465: train loss: 0.006561018526554108\n",
      "Epoch 1466: train loss: 0.006556827574968338\n",
      "Epoch 1467: train loss: 0.0065526640973985195\n",
      "Epoch 1468: train loss: 0.006548480596393347\n",
      "Epoch 1469: train loss: 0.006544309668242931\n",
      "Epoch 1470: train loss: 0.006540139205753803\n",
      "Epoch 1471: train loss: 0.006535968743264675\n",
      "Epoch 1472: train loss: 0.006531818304210901\n",
      "Epoch 1473: train loss: 0.006527668330818415\n",
      "Epoch 1474: train loss: 0.00652353186160326\n",
      "Epoch 1475: train loss: 0.006519387010484934\n",
      "Epoch 1476: train loss: 0.00651524867862463\n",
      "Epoch 1477: train loss: 0.006511128041893244\n",
      "Epoch 1478: train loss: 0.006507010664790869\n",
      "Epoch 1479: train loss: 0.006502883043140173\n",
      "Epoch 1480: train loss: 0.006498777307569981\n",
      "Epoch 1481: train loss: 0.006494675762951374\n",
      "Epoch 1482: train loss: 0.006490563508123159\n",
      "Epoch 1483: train loss: 0.006486475467681885\n",
      "Epoch 1484: train loss: 0.006482372526079416\n",
      "Epoch 1485: train loss: 0.006478294730186462\n",
      "Epoch 1486: train loss: 0.006474217865616083\n",
      "Epoch 1487: train loss: 0.006470139138400555\n",
      "Epoch 1488: train loss: 0.006466073449701071\n",
      "Epoch 1489: train loss: 0.0064620026387274265\n",
      "Epoch 1490: train loss: 0.006457940675318241\n",
      "Epoch 1491: train loss: 0.00645389873534441\n",
      "Epoch 1492: train loss: 0.006449844688177109\n",
      "Epoch 1493: train loss: 0.0064458041451871395\n",
      "Epoch 1494: train loss: 0.006441754288971424\n",
      "Epoch 1495: train loss: 0.006437727715820074\n",
      "Epoch 1496: train loss: 0.0064336927607655525\n",
      "Epoch 1497: train loss: 0.00642967876046896\n",
      "Epoch 1498: train loss: 0.006425660103559494\n",
      "Epoch 1499: train loss: 0.006421644240617752\n",
      "Epoch 1500: train loss: 0.006417649798095226\n",
      "Epoch 1501: train loss: 0.006413638591766357\n",
      "Epoch 1502: train loss: 0.0064096515998244286\n",
      "Epoch 1503: train loss: 0.00640565762296319\n",
      "Epoch 1504: train loss: 0.006401659455150366\n",
      "Epoch 1505: train loss: 0.006397696677595377\n",
      "Epoch 1506: train loss: 0.006393712945282459\n",
      "Epoch 1507: train loss: 0.006389749702066183\n",
      "Epoch 1508: train loss: 0.006385785527527332\n",
      "Epoch 1509: train loss: 0.006381819490343332\n",
      "Epoch 1510: train loss: 0.006377863697707653\n",
      "Epoch 1511: train loss: 0.006373908370733261\n",
      "Epoch 1512: train loss: 0.0063699716702103615\n",
      "Epoch 1513: train loss: 0.006366015877574682\n",
      "Epoch 1514: train loss: 0.0063620926812291145\n",
      "Epoch 1515: train loss: 0.006358156446367502\n",
      "Epoch 1516: train loss: 0.006354229059070349\n",
      "Epoch 1517: train loss: 0.006350310984998941\n",
      "Epoch 1518: train loss: 0.006346393376588821\n",
      "Epoch 1519: train loss: 0.006342482753098011\n",
      "Epoch 1520: train loss: 0.006338582374155521\n",
      "Epoch 1521: train loss: 0.006334672681987286\n",
      "Epoch 1522: train loss: 0.006330775562673807\n",
      "Epoch 1523: train loss: 0.00632689381018281\n",
      "Epoch 1524: train loss: 0.0063230060040950775\n",
      "Epoch 1525: train loss: 0.006319129373878241\n",
      "Epoch 1526: train loss: 0.006315248552709818\n",
      "Epoch 1527: train loss: 0.006311381701380014\n",
      "Epoch 1528: train loss: 0.006307510193437338\n",
      "Epoch 1529: train loss: 0.006303647067397833\n",
      "Epoch 1530: train loss: 0.006299794651567936\n",
      "Epoch 1531: train loss: 0.006295929662883282\n",
      "Epoch 1532: train loss: 0.006292092148214579\n",
      "Epoch 1533: train loss: 0.006288259290158749\n",
      "Epoch 1534: train loss: 0.006284414790570736\n",
      "Epoch 1535: train loss: 0.006280583329498768\n",
      "Epoch 1536: train loss: 0.0062767500057816505\n",
      "Epoch 1537: train loss: 0.006272925529628992\n",
      "Epoch 1538: train loss: 0.0062691099010407925\n",
      "Epoch 1539: train loss: 0.006265288684517145\n",
      "Epoch 1540: train loss: 0.006261484231799841\n",
      "Epoch 1541: train loss: 0.006257690489292145\n",
      "Epoch 1542: train loss: 0.006253896746784449\n",
      "Epoch 1543: train loss: 0.006250091828405857\n",
      "Epoch 1544: train loss: 0.006246308796107769\n",
      "Epoch 1545: train loss: 0.006242520175874233\n",
      "Epoch 1546: train loss: 0.006238738540560007\n",
      "Epoch 1547: train loss: 0.006234961561858654\n",
      "Epoch 1548: train loss: 0.006231201812624931\n",
      "Epoch 1549: train loss: 0.006227424368262291\n",
      "Epoch 1550: train loss: 0.006223668344318867\n",
      "Epoch 1551: train loss: 0.006219914183020592\n",
      "Epoch 1552: train loss: 0.00621615257114172\n",
      "Epoch 1553: train loss: 0.00621240446344018\n",
      "Epoch 1554: train loss: 0.006208666134625673\n",
      "Epoch 1555: train loss: 0.0062049273401498795\n",
      "Epoch 1556: train loss: 0.006201193667948246\n",
      "Epoch 1557: train loss: 0.006197469308972359\n",
      "Epoch 1558: train loss: 0.006193747743964195\n",
      "Epoch 1559: train loss: 0.006190028041601181\n",
      "Epoch 1560: train loss: 0.006186319049447775\n",
      "Epoch 1561: train loss: 0.006182609125971794\n",
      "Epoch 1562: train loss: 0.006178897339850664\n",
      "Epoch 1563: train loss: 0.0061751920729875565\n",
      "Epoch 1564: train loss: 0.006171492859721184\n",
      "Epoch 1565: train loss: 0.0061677987687289715\n",
      "Epoch 1566: train loss: 0.006164115853607655\n",
      "Epoch 1567: train loss: 0.006160435266792774\n",
      "Epoch 1568: train loss: 0.00615675887092948\n",
      "Epoch 1569: train loss: 0.006153087131679058\n",
      "Epoch 1570: train loss: 0.006149405613541603\n",
      "Epoch 1571: train loss: 0.006145750638097525\n",
      "Epoch 1572: train loss: 0.006142078433185816\n",
      "Epoch 1573: train loss: 0.006138433236628771\n",
      "Epoch 1574: train loss: 0.00613477872684598\n",
      "Epoch 1575: train loss: 0.006131125148385763\n",
      "Epoch 1576: train loss: 0.006127490662038326\n",
      "Epoch 1577: train loss: 0.006123853847384453\n",
      "Epoch 1578: train loss: 0.00612022215500474\n",
      "Epoch 1579: train loss: 0.00611658301204443\n",
      "Epoch 1580: train loss: 0.006112962029874325\n",
      "Epoch 1581: train loss: 0.00610934104770422\n",
      "Epoch 1582: train loss: 0.006105723790824413\n",
      "Epoch 1583: train loss: 0.006102121435105801\n",
      "Epoch 1584: train loss: 0.006098504643887281\n",
      "Epoch 1585: train loss: 0.006094901356846094\n",
      "Epoch 1586: train loss: 0.006091306451708078\n",
      "Epoch 1587: train loss: 0.006087702699005604\n",
      "Epoch 1588: train loss: 0.006084125023335218\n",
      "Epoch 1589: train loss: 0.0060805208049714565\n",
      "Epoch 1590: train loss: 0.00607695709913969\n",
      "Epoch 1591: train loss: 0.006073378026485443\n",
      "Epoch 1592: train loss: 0.006069790571928024\n",
      "Epoch 1593: train loss: 0.006066232454031706\n",
      "Epoch 1594: train loss: 0.00606266874819994\n",
      "Epoch 1595: train loss: 0.006059106905013323\n",
      "Epoch 1596: train loss: 0.0060555534437298775\n",
      "Epoch 1597: train loss: 0.006052009295672178\n",
      "Epoch 1598: train loss: 0.006048446986824274\n",
      "Epoch 1599: train loss: 0.0060449098236858845\n",
      "Epoch 1600: train loss: 0.006041381973773241\n",
      "Epoch 1601: train loss: 0.006037840154021978\n",
      "Epoch 1602: train loss: 0.006034315563738346\n",
      "Epoch 1603: train loss: 0.006030786782503128\n",
      "Epoch 1604: train loss: 0.006027263589203358\n",
      "Epoch 1605: train loss: 0.00602375203743577\n",
      "Epoch 1606: train loss: 0.006020231172442436\n",
      "Epoch 1607: train loss: 0.006016724742949009\n",
      "Epoch 1608: train loss: 0.006013217847794294\n",
      "Epoch 1609: train loss: 0.006009729113429785\n",
      "Epoch 1610: train loss: 0.006006221286952496\n",
      "Epoch 1611: train loss: 0.006002741865813732\n",
      "Epoch 1612: train loss: 0.005999238695949316\n",
      "Epoch 1613: train loss: 0.00599575974047184\n",
      "Epoch 1614: train loss: 0.005992278456687927\n",
      "Epoch 1615: train loss: 0.00598880322650075\n",
      "Epoch 1616: train loss: 0.005985329858958721\n",
      "Epoch 1617: train loss: 0.005981859751045704\n",
      "Epoch 1618: train loss: 0.005978413391858339\n",
      "Epoch 1619: train loss: 0.005974952131509781\n",
      "Epoch 1620: train loss: 0.005971476435661316\n",
      "Epoch 1621: train loss: 0.005968036595731974\n",
      "Epoch 1622: train loss: 0.005964594893157482\n",
      "Epoch 1623: train loss: 0.005961152724921703\n",
      "Epoch 1624: train loss: 0.005957707297056913\n",
      "Epoch 1625: train loss: 0.005954277701675892\n",
      "Epoch 1626: train loss: 0.005950842518359423\n",
      "Epoch 1627: train loss: 0.005947427824139595\n",
      "Epoch 1628: train loss: 0.005943997763097286\n",
      "Epoch 1629: train loss: 0.005940577015280724\n",
      "Epoch 1630: train loss: 0.005937163718044758\n",
      "Epoch 1631: train loss: 0.005933746695518494\n",
      "Epoch 1632: train loss: 0.005930344108492136\n",
      "Epoch 1633: train loss: 0.005926938261836767\n",
      "Epoch 1634: train loss: 0.005923540331423283\n",
      "Epoch 1635: train loss: 0.005920149851590395\n",
      "Epoch 1636: train loss: 0.0059167612344026566\n",
      "Epoch 1637: train loss: 0.0059133716858923435\n",
      "Epoch 1638: train loss: 0.005909980274736881\n",
      "Epoch 1639: train loss: 0.0059066107496619225\n",
      "Epoch 1640: train loss: 0.005903238896280527\n",
      "Epoch 1641: train loss: 0.005899866111576557\n",
      "Epoch 1642: train loss: 0.0058965012431144714\n",
      "Epoch 1643: train loss: 0.005893136374652386\n",
      "Epoch 1644: train loss: 0.005889775697141886\n",
      "Epoch 1645: train loss: 0.005886420141905546\n",
      "Epoch 1646: train loss: 0.005883075762540102\n",
      "Epoch 1647: train loss: 0.00587971368804574\n",
      "Epoch 1648: train loss: 0.005876378621906042\n",
      "Epoch 1649: train loss: 0.005873037967830896\n",
      "Epoch 1650: train loss: 0.005869694519788027\n",
      "Epoch 1651: train loss: 0.0058663710951805115\n",
      "Epoch 1652: train loss: 0.005863037891685963\n",
      "Epoch 1653: train loss: 0.005859714932739735\n",
      "Epoch 1654: train loss: 0.00585640175268054\n",
      "Epoch 1655: train loss: 0.005853085312992334\n",
      "Epoch 1656: train loss: 0.005849767010658979\n",
      "Epoch 1657: train loss: 0.005846467800438404\n",
      "Epoch 1658: train loss: 0.0058431620709598064\n",
      "Epoch 1659: train loss: 0.005839860998094082\n",
      "Epoch 1660: train loss: 0.005836564116179943\n",
      "Epoch 1661: train loss: 0.005833274684846401\n",
      "Epoch 1662: train loss: 0.00582998339086771\n",
      "Epoch 1663: train loss: 0.005826693028211594\n",
      "Epoch 1664: train loss: 0.005823412444442511\n",
      "Epoch 1665: train loss: 0.00582012627273798\n",
      "Epoch 1666: train loss: 0.0058168573305010796\n",
      "Epoch 1667: train loss: 0.005813593044877052\n",
      "Epoch 1668: train loss: 0.0058103324845433235\n",
      "Epoch 1669: train loss: 0.005807059817016125\n",
      "Epoch 1670: train loss: 0.005803802516311407\n",
      "Epoch 1671: train loss: 0.0058005270548164845\n",
      "Epoch 1672: train loss: 0.005797293037176132\n",
      "Epoch 1673: train loss: 0.005794039927423\n",
      "Epoch 1674: train loss: 0.005790797993540764\n",
      "Epoch 1675: train loss: 0.005787556059658527\n",
      "Epoch 1676: train loss: 0.0057843271642923355\n",
      "Epoch 1677: train loss: 0.005781097337603569\n",
      "Epoch 1678: train loss: 0.005777858663350344\n",
      "Epoch 1679: train loss: 0.005774634890258312\n",
      "Epoch 1680: train loss: 0.00577140599489212\n",
      "Epoch 1681: train loss: 0.005768187809735537\n",
      "Epoch 1682: train loss: 0.00576497707515955\n",
      "Epoch 1683: train loss: 0.005761767737567425\n",
      "Epoch 1684: train loss: 0.005758550949394703\n",
      "Epoch 1685: train loss: 0.005755346268415451\n",
      "Epoch 1686: train loss: 0.005752147175371647\n",
      "Epoch 1687: train loss: 0.0057489448226988316\n",
      "Epoch 1688: train loss: 0.005745763890445232\n",
      "Epoch 1689: train loss: 0.0057425787672400475\n",
      "Epoch 1690: train loss: 0.005739390384405851\n",
      "Epoch 1691: train loss: 0.005736207123845816\n",
      "Epoch 1692: train loss: 0.00573302898555994\n",
      "Epoch 1693: train loss: 0.0057298531755805016\n",
      "Epoch 1694: train loss: 0.005726682022213936\n",
      "Epoch 1695: train loss: 0.0057235052809119225\n",
      "Epoch 1696: train loss: 0.005720349960029125\n",
      "Epoch 1697: train loss: 0.005717199761420488\n",
      "Epoch 1698: train loss: 0.005714030936360359\n",
      "Epoch 1699: train loss: 0.005710874684154987\n",
      "Epoch 1700: train loss: 0.005707724019885063\n",
      "Epoch 1701: train loss: 0.00570457661524415\n",
      "Epoch 1702: train loss: 0.005701431073248386\n",
      "Epoch 1703: train loss: 0.0056982929818332195\n",
      "Epoch 1704: train loss: 0.005695166066288948\n",
      "Epoch 1705: train loss: 0.0056920223869383335\n",
      "Epoch 1706: train loss: 0.005688898731023073\n",
      "Epoch 1707: train loss: 0.005685776472091675\n",
      "Epoch 1708: train loss: 0.005682649090886116\n",
      "Epoch 1709: train loss: 0.0056795296259224415\n",
      "Epoch 1710: train loss: 0.005676421336829662\n",
      "Epoch 1711: train loss: 0.005673310253769159\n",
      "Epoch 1712: train loss: 0.005670198239386082\n",
      "Epoch 1713: train loss: 0.005667093675583601\n",
      "Epoch 1714: train loss: 0.005663995631039143\n",
      "Epoch 1715: train loss: 0.005660888273268938\n",
      "Epoch 1716: train loss: 0.005657798144966364\n",
      "Epoch 1717: train loss: 0.005654704757034779\n",
      "Epoch 1718: train loss: 0.005651617422699928\n",
      "Epoch 1719: train loss: 0.005648544989526272\n",
      "Epoch 1720: train loss: 0.005645458586513996\n",
      "Epoch 1721: train loss: 0.005642376374453306\n",
      "Epoch 1722: train loss: 0.005639305338263512\n",
      "Epoch 1723: train loss: 0.005636229179799557\n",
      "Epoch 1724: train loss: 0.005633165594190359\n",
      "Epoch 1725: train loss: 0.005630111787468195\n",
      "Epoch 1726: train loss: 0.005627053324133158\n",
      "Epoch 1727: train loss: 0.005623993463814259\n",
      "Epoch 1728: train loss: 0.005620930809527636\n",
      "Epoch 1729: train loss: 0.005617889575660229\n",
      "Epoch 1730: train loss: 0.005614834371954203\n",
      "Epoch 1731: train loss: 0.0056117926724255085\n",
      "Epoch 1732: train loss: 0.005608743987977505\n",
      "Epoch 1733: train loss: 0.0056057278998196125\n",
      "Epoch 1734: train loss: 0.005602699238806963\n",
      "Epoch 1735: train loss: 0.005599655210971832\n",
      "Epoch 1736: train loss: 0.005596628878265619\n",
      "Epoch 1737: train loss: 0.005593606736510992\n",
      "Epoch 1738: train loss: 0.0055905906483531\n",
      "Epoch 1739: train loss: 0.005587574560195208\n",
      "Epoch 1740: train loss: 0.005584570579230785\n",
      "Epoch 1741: train loss: 0.005581540055572987\n",
      "Epoch 1742: train loss: 0.0055785467848181725\n",
      "Epoch 1743: train loss: 0.005575540475547314\n",
      "Epoch 1744: train loss: 0.005572534631937742\n",
      "Epoch 1745: train loss: 0.00556953763589263\n",
      "Epoch 1746: train loss: 0.005566549487411976\n",
      "Epoch 1747: train loss: 0.005563555750995874\n",
      "Epoch 1748: train loss: 0.005560571327805519\n",
      "Epoch 1749: train loss: 0.005557584576308727\n",
      "Epoch 1750: train loss: 0.0055546024814248085\n",
      "Epoch 1751: train loss: 0.005551627837121487\n",
      "Epoch 1752: train loss: 0.005548653192818165\n",
      "Epoch 1753: train loss: 0.005545689724385738\n",
      "Epoch 1754: train loss: 0.005542716942727566\n",
      "Epoch 1755: train loss: 0.0055397553369402885\n",
      "Epoch 1756: train loss: 0.00553679745644331\n",
      "Epoch 1757: train loss: 0.005533844698220491\n",
      "Epoch 1758: train loss: 0.005530881695449352\n",
      "Epoch 1759: train loss: 0.00552793312817812\n",
      "Epoch 1760: train loss: 0.005524983163923025\n",
      "Epoch 1761: train loss: 0.0055220359936356544\n",
      "Epoch 1762: train loss: 0.005519095808267593\n",
      "Epoch 1763: train loss: 0.005516155622899532\n",
      "Epoch 1764: train loss: 0.005513226147741079\n",
      "Epoch 1765: train loss: 0.005510282702744007\n",
      "Epoch 1766: train loss: 0.005507352761924267\n",
      "Epoch 1767: train loss: 0.0055044363252818584\n",
      "Epoch 1768: train loss: 0.005501506384462118\n",
      "Epoch 1769: train loss: 0.005498598329722881\n",
      "Epoch 1770: train loss: 0.005495673045516014\n",
      "Epoch 1771: train loss: 0.0054927729070186615\n",
      "Epoch 1772: train loss: 0.005489854607731104\n",
      "Epoch 1773: train loss: 0.005486949346959591\n",
      "Epoch 1774: train loss: 0.005484042223542929\n",
      "Epoch 1775: train loss: 0.0054811351001262665\n",
      "Epoch 1776: train loss: 0.005478250328451395\n",
      "Epoch 1777: train loss: 0.005475351586937904\n",
      "Epoch 1778: train loss: 0.005472454242408276\n",
      "Epoch 1779: train loss: 0.005469573196023703\n",
      "Epoch 1780: train loss: 0.00546668516471982\n",
      "Epoch 1781: train loss: 0.005463799927383661\n",
      "Epoch 1782: train loss: 0.005460923071950674\n",
      "Epoch 1783: train loss: 0.0054580471478402615\n",
      "Epoch 1784: train loss: 0.0054551768116652966\n",
      "Epoch 1785: train loss: 0.005452305544167757\n",
      "Epoch 1786: train loss: 0.005449437536299229\n",
      "Epoch 1787: train loss: 0.005446576047688723\n",
      "Epoch 1788: train loss: 0.005443706177175045\n",
      "Epoch 1789: train loss: 0.005440858192741871\n",
      "Epoch 1790: train loss: 0.005437999963760376\n",
      "Epoch 1791: train loss: 0.005435149185359478\n",
      "Epoch 1792: train loss: 0.005432306323200464\n",
      "Epoch 1793: train loss: 0.005429456941783428\n",
      "Epoch 1794: train loss: 0.005426615010946989\n",
      "Epoch 1795: train loss: 0.005423783324658871\n",
      "Epoch 1796: train loss: 0.005420931614935398\n",
      "Epoch 1797: train loss: 0.005418103188276291\n",
      "Epoch 1798: train loss: 0.005415280349552631\n",
      "Epoch 1799: train loss: 0.0054124491289258\n",
      "Epoch 1800: train loss: 0.005409617908298969\n",
      "Epoch 1801: train loss: 0.005406802520155907\n",
      "Epoch 1802: train loss: 0.005403977818787098\n",
      "Epoch 1803: train loss: 0.005401161499321461\n",
      "Epoch 1804: train loss: 0.005398370325565338\n",
      "Epoch 1805: train loss: 0.005395543295890093\n",
      "Epoch 1806: train loss: 0.00539273489266634\n",
      "Epoch 1807: train loss: 0.005389942321926355\n",
      "Epoch 1808: train loss: 0.005387145094573498\n",
      "Epoch 1809: train loss: 0.0053843422792851925\n",
      "Epoch 1810: train loss: 0.0053815389983356\n",
      "Epoch 1811: train loss: 0.005378758069127798\n",
      "Epoch 1812: train loss: 0.005375967361032963\n",
      "Epoch 1813: train loss: 0.00537318317219615\n",
      "Epoch 1814: train loss: 0.005370402242988348\n",
      "Epoch 1815: train loss: 0.005367618519812822\n",
      "Epoch 1816: train loss: 0.0053648436442017555\n",
      "Epoch 1817: train loss: 0.0053620669059455395\n",
      "Epoch 1818: train loss: 0.005359297152608633\n",
      "Epoch 1819: train loss: 0.005356533918529749\n",
      "Epoch 1820: train loss: 0.005353768356144428\n",
      "Epoch 1821: train loss: 0.005351003725081682\n",
      "Epoch 1822: train loss: 0.005348237231373787\n",
      "Epoch 1823: train loss: 0.005345472134649754\n",
      "Epoch 1824: train loss: 0.005342722870409489\n",
      "Epoch 1825: train loss: 0.005339975468814373\n",
      "Epoch 1826: train loss: 0.005337237846106291\n",
      "Epoch 1827: train loss: 0.005334489978849888\n",
      "Epoch 1828: train loss: 0.005331731401383877\n",
      "Epoch 1829: train loss: 0.005328997038304806\n",
      "Epoch 1830: train loss: 0.005326261278241873\n",
      "Epoch 1831: train loss: 0.005323524121195078\n",
      "Epoch 1832: train loss: 0.005320795811712742\n",
      "Epoch 1833: train loss: 0.005318081472069025\n",
      "Epoch 1834: train loss: 0.005315348505973816\n",
      "Epoch 1835: train loss: 0.005312626250088215\n",
      "Epoch 1836: train loss: 0.005309903994202614\n",
      "Epoch 1837: train loss: 0.005307180806994438\n",
      "Epoch 1838: train loss: 0.005304470658302307\n",
      "Epoch 1839: train loss: 0.005301761440932751\n",
      "Epoch 1840: train loss: 0.00529905129224062\n",
      "Epoch 1841: train loss: 0.005296353250741959\n",
      "Epoch 1842: train loss: 0.005293642170727253\n",
      "Epoch 1843: train loss: 0.00529094971716404\n",
      "Epoch 1844: train loss: 0.005288246553391218\n",
      "Epoch 1845: train loss: 0.005285562481731176\n",
      "Epoch 1846: train loss: 0.0052828676998615265\n",
      "Epoch 1847: train loss: 0.005280178971588612\n",
      "Epoch 1848: train loss: 0.005277497693896294\n",
      "Epoch 1849: train loss: 0.005274809896945953\n",
      "Epoch 1850: train loss: 0.0052721272222697735\n",
      "Epoch 1851: train loss: 0.005269449669867754\n",
      "Epoch 1852: train loss: 0.0052667828276753426\n",
      "Epoch 1853: train loss: 0.005264102015644312\n",
      "Epoch 1854: train loss: 0.005261424463242292\n",
      "Epoch 1855: train loss: 0.005258763674646616\n",
      "Epoch 1856: train loss: 0.005256095435470343\n",
      "Epoch 1857: train loss: 0.0052534411661326885\n",
      "Epoch 1858: train loss: 0.0052507733926177025\n",
      "Epoch 1859: train loss: 0.0052481284365057945\n",
      "Epoch 1860: train loss: 0.0052454592660069466\n",
      "Epoch 1861: train loss: 0.0052428278140723705\n",
      "Epoch 1862: train loss: 0.005240167491137981\n",
      "Epoch 1863: train loss: 0.0052375285886228085\n",
      "Epoch 1864: train loss: 0.005234886892139912\n",
      "Epoch 1865: train loss: 0.005232235882431269\n",
      "Epoch 1866: train loss: 0.005229614209383726\n",
      "Epoch 1867: train loss: 0.005226968787610531\n",
      "Epoch 1868: train loss: 0.005224342457950115\n",
      "Epoch 1869: train loss: 0.005221712402999401\n",
      "Epoch 1870: train loss: 0.005219093058258295\n",
      "Epoch 1871: train loss: 0.005216467194259167\n",
      "Epoch 1872: train loss: 0.005213839001953602\n",
      "Epoch 1873: train loss: 0.005211224313825369\n",
      "Epoch 1874: train loss: 0.00520861241966486\n",
      "Epoch 1875: train loss: 0.0052059912122786045\n",
      "Epoch 1876: train loss: 0.00520338723435998\n",
      "Epoch 1877: train loss: 0.005200778134167194\n",
      "Epoch 1878: train loss: 0.005198169965296984\n",
      "Epoch 1879: train loss: 0.005195567850023508\n",
      "Epoch 1880: train loss: 0.005192965269088745\n",
      "Epoch 1881: train loss: 0.005190378520637751\n",
      "Epoch 1882: train loss: 0.005187778268009424\n",
      "Epoch 1883: train loss: 0.005185195244848728\n",
      "Epoch 1884: train loss: 0.005182592663913965\n",
      "Epoch 1885: train loss: 0.005180012434720993\n",
      "Epoch 1886: train loss: 0.005177413113415241\n",
      "Epoch 1887: train loss: 0.0051748366095125675\n",
      "Epoch 1888: train loss: 0.005172261036932468\n",
      "Epoch 1889: train loss: 0.00516969058662653\n",
      "Epoch 1890: train loss: 0.005167120601981878\n",
      "Epoch 1891: train loss: 0.005164542235434055\n",
      "Epoch 1892: train loss: 0.005161967594176531\n",
      "Epoch 1893: train loss: 0.0051594083197414875\n",
      "Epoch 1894: train loss: 0.005156845785677433\n",
      "Epoch 1895: train loss: 0.0051542832516133785\n",
      "Epoch 1896: train loss: 0.005151717457920313\n",
      "Epoch 1897: train loss: 0.0051491702906787395\n",
      "Epoch 1898: train loss: 0.005146618001163006\n",
      "Epoch 1899: train loss: 0.005144059658050537\n",
      "Epoch 1900: train loss: 0.005141519010066986\n",
      "Epoch 1901: train loss: 0.00513897929340601\n",
      "Epoch 1902: train loss: 0.005136427935212851\n",
      "Epoch 1903: train loss: 0.00513389240950346\n",
      "Epoch 1904: train loss: 0.005131355486810207\n",
      "Epoch 1905: train loss: 0.005128811579197645\n",
      "Epoch 1906: train loss: 0.005126288626343012\n",
      "Epoch 1907: train loss: 0.005123752169311047\n",
      "Epoch 1908: train loss: 0.0051212250255048275\n",
      "Epoch 1909: train loss: 0.005118691362440586\n",
      "Epoch 1910: train loss: 0.005116176791489124\n",
      "Epoch 1911: train loss: 0.005113665480166674\n",
      "Epoch 1912: train loss: 0.005111129023134708\n",
      "Epoch 1913: train loss: 0.005108614452183247\n",
      "Epoch 1914: train loss: 0.005106116645038128\n",
      "Epoch 1915: train loss: 0.0051035950891673565\n",
      "Epoch 1916: train loss: 0.005101092159748077\n",
      "Epoch 1917: train loss: 0.005098601337522268\n",
      "Epoch 1918: train loss: 0.005096083041280508\n",
      "Epoch 1919: train loss: 0.0050935810431838036\n",
      "Epoch 1920: train loss: 0.005091087426990271\n",
      "Epoch 1921: train loss: 0.00508858310058713\n",
      "Epoch 1922: train loss: 0.00508609414100647\n",
      "Epoch 1923: train loss: 0.0050836047157645226\n",
      "Epoch 1924: train loss: 0.005081114359200001\n",
      "Epoch 1925: train loss: 0.005078636575490236\n",
      "Epoch 1926: train loss: 0.005076155066490173\n",
      "Epoch 1927: train loss: 0.005073672626167536\n",
      "Epoch 1928: train loss: 0.005071195773780346\n",
      "Epoch 1929: train loss: 0.0050687165930867195\n",
      "Epoch 1930: train loss: 0.005066252313554287\n",
      "Epoch 1931: train loss: 0.0050637805834412575\n",
      "Epoch 1932: train loss: 0.005061306990683079\n",
      "Epoch 1933: train loss: 0.005058842245489359\n",
      "Epoch 1934: train loss: 0.005056374706327915\n",
      "Epoch 1935: train loss: 0.0050539192743599415\n",
      "Epoch 1936: train loss: 0.005051459185779095\n",
      "Epoch 1937: train loss: 0.005049009807407856\n",
      "Epoch 1938: train loss: 0.0050465441308915615\n",
      "Epoch 1939: train loss: 0.005044104065746069\n",
      "Epoch 1940: train loss: 0.005041648633778095\n",
      "Epoch 1941: train loss: 0.005039204843342304\n",
      "Epoch 1942: train loss: 0.0050367554649710655\n",
      "Epoch 1943: train loss: 0.0050343107432127\n",
      "Epoch 1944: train loss: 0.005031878128647804\n",
      "Epoch 1945: train loss: 0.005029430612921715\n",
      "Epoch 1946: train loss: 0.005026994738727808\n",
      "Epoch 1947: train loss: 0.0050245714373886585\n",
      "Epoch 1948: train loss: 0.005022142548114061\n",
      "Epoch 1949: train loss: 0.005019707139581442\n",
      "Epoch 1950: train loss: 0.005017288960516453\n",
      "Epoch 1951: train loss: 0.005014863796532154\n",
      "Epoch 1952: train loss: 0.005012446083128452\n",
      "Epoch 1953: train loss: 0.005010027438402176\n",
      "Epoch 1954: train loss: 0.005007610656321049\n",
      "Epoch 1955: train loss: 0.005005192942917347\n",
      "Epoch 1956: train loss: 0.005002782680094242\n",
      "Epoch 1957: train loss: 0.00500037195160985\n",
      "Epoch 1958: train loss: 0.004997970536351204\n",
      "Epoch 1959: train loss: 0.004995564464479685\n",
      "Epoch 1960: train loss: 0.004993157461285591\n",
      "Epoch 1961: train loss: 0.004990764893591404\n",
      "Epoch 1962: train loss: 0.004988354630768299\n",
      "Epoch 1963: train loss: 0.004985963460057974\n",
      "Epoch 1964: train loss: 0.004983581602573395\n",
      "Epoch 1965: train loss: 0.004981192294508219\n",
      "Epoch 1966: train loss: 0.004978791810572147\n",
      "Epoch 1967: train loss: 0.004976410418748856\n",
      "Epoch 1968: train loss: 0.004974033683538437\n",
      "Epoch 1969: train loss: 0.004971652291715145\n",
      "Epoch 1970: train loss: 0.0049692606553435326\n",
      "Epoch 1971: train loss: 0.00496688112616539\n",
      "Epoch 1972: train loss: 0.0049645183607935905\n",
      "Epoch 1973: train loss: 0.004962150938808918\n",
      "Epoch 1974: train loss: 0.004959772806614637\n",
      "Epoch 1975: train loss: 0.00495740445330739\n",
      "Epoch 1976: train loss: 0.004955045413225889\n",
      "Epoch 1977: train loss: 0.004952680319547653\n",
      "Epoch 1978: train loss: 0.004950318019837141\n",
      "Epoch 1979: train loss: 0.004947963170707226\n",
      "Epoch 1980: train loss: 0.0049456143751740456\n",
      "Epoch 1981: train loss: 0.004943256266415119\n",
      "Epoch 1982: train loss: 0.004940897226333618\n",
      "Epoch 1983: train loss: 0.004938557744026184\n",
      "Epoch 1984: train loss: 0.004936215467751026\n",
      "Epoch 1985: train loss: 0.004933874122798443\n",
      "Epoch 1986: train loss: 0.004931528586894274\n",
      "Epoch 1987: train loss: 0.004929183982312679\n",
      "Epoch 1988: train loss: 0.004926852881908417\n",
      "Epoch 1989: train loss: 0.004924519918859005\n",
      "Epoch 1990: train loss: 0.004922168329358101\n",
      "Epoch 1991: train loss: 0.004919847007840872\n",
      "Epoch 1992: train loss: 0.00491752102971077\n",
      "Epoch 1993: train loss: 0.004915196914225817\n",
      "Epoch 1994: train loss: 0.004912858363240957\n",
      "Epoch 1995: train loss: 0.004910540767014027\n",
      "Epoch 1996: train loss: 0.004908219911158085\n",
      "Epoch 1997: train loss: 0.004905899055302143\n",
      "Epoch 1998: train loss: 0.004903593100607395\n",
      "Epoch 1999: train loss: 0.004901274107396603\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 2000\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(x_train)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y_train)\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 3.8493354320526123\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(x_test)\n",
    "after_train = criterion(y_pred.squeeze(), y_test) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0161],\n",
       "        [0.9866],\n",
       "        [1.0000],\n",
       "        [0.9708],\n",
       "        [0.1587],\n",
       "        [0.9981],\n",
       "        [0.0484],\n",
       "        [0.9778],\n",
       "        [0.9907],\n",
       "        [0.9999]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1., 1., 0., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
